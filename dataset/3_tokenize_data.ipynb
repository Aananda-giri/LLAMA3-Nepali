{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_fc7VvAINALZ",
        "outputId": "820f1b0a-ca39-48a9-b435-847c0434c94d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "/content/drive/MyDrive/Research/dataset/hf_dataset\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Research/dataset/hf_dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i6NY1b4tNEb7",
        "outputId": "e3dc044a-626c-4a98-9cad-df2f93f7cdc8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'0. download_data.ipynb'     clean_date_categories.csv\t\t iriis_text.txt      tokenize\n",
            "'1. train_tokenizer.ipynb'   clean_nepberta_data.zip\t\t NepaliBPE\t     vocab_old.json\n",
            "'3. tokenize_data.ipynb'     IRIISNEPAL_Nepali_Text_Corpus.csv\t nepberta_text.txt   vocab_old.text\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 0. pseudocode.\n",
        "```\n",
        "def tokens_generator():\n",
        "  * for line in lines of text file\n",
        "    * tokens=tokenize(line)\n",
        "    * yield [token for token in tokens]\n",
        "\n",
        "def collector(max_len)\n",
        "  repeat generator stops giving tokens:\n",
        "  while len(collected_tokens) < max_len+1:\n",
        "    * collected_tokens.append(collect from tokens_generator_function)\n",
        "    * get input_tokens, target_tokens and append to csv\n",
        "    * collected_tokens = collected_tokens[stride:] # preserving tokens of previous data_item for next data_item\n",
        "```"
      ],
      "metadata": {
        "id": "NEagv29e4yMh"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mDA3AsKN1feM"
      },
      "source": [
        "## 1. Testing pre-tokenization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NPCT0tZhRDOb",
        "outputId": "54827db1-abcd-4c52-9392-14f7a9d81c29"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'one two three'"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class TestTokenizer():\n",
        "  def __init__(self):\n",
        "    self.vocab = {\n",
        "        'zero': 0,\n",
        "        'one': 1,\n",
        "        'two': 2,\n",
        "        'three': 3,\n",
        "        'four': 4,\n",
        "        'five': 5,\n",
        "        'six': 6,\n",
        "        'seven': 7,\n",
        "        'eight': 8,\n",
        "        'nine': 9,\n",
        "        'ten': 10,\n",
        "        'eleven': 11,\n",
        "        'twelve': 12,\n",
        "        'thirteen': 13,\n",
        "        'fourteen': 14,\n",
        "        'fifteen':15\n",
        "    }\n",
        "  def encode(self, text, allowed_special=None):\n",
        "    return [self.vocab[word] for word in text.split()]\n",
        "\n",
        "  def decode(self, tokens):\n",
        "    return ' '.join([list(self.vocab.keys())[list(self.vocab.values()).index(token)] for token in tokens])\n",
        "\n",
        "tokenizer=TestTokenizer()\n",
        "tokenizer.encode('one two three')\n",
        "tokenizer.decode([1,2,3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2eG4-XOWpMTI",
        "outputId": "a8e6662c-bbcb-4743-c47f-e65317718443"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "file: file1.txt\n",
            "Output file: creating new: tokenized_data_test.csv\n",
            "End of file reached.\n",
            "file: file2.txt\n",
            "Output file exists: tokenized_data_test.csv\n",
            "End of file reached.\n",
            "time taken:0.0003130078315734863 hours\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "# from transformers import GPT2Tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "class LargeFileTokenizer:\n",
        "    def __init__(self, tokenizer, max_length, stride, input_file, output_file):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.stride = stride\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "        print(f'file: {self.input_file}')\n",
        "\n",
        "    def tokens_generator(self):\n",
        "        \"\"\"Yields tokenized lines from a large text file.\"\"\"\n",
        "        with open(self.input_file, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                # Tokenize the line and allow special tokens\n",
        "                tokens = self.tokenizer.encode(line.strip(), allowed_special={'<|endoftext|>'})\n",
        "                for token in tokens:\n",
        "                    yield token\n",
        "\n",
        "    def collect_and_save(self):\n",
        "        \"\"\"Collects tokens into chunks and saves them to a CSV file.\"\"\"\n",
        "        collected_tokens = []\n",
        "        token_gen = self.tokens_generator()\n",
        "\n",
        "        # # Open the CSV file to save tokenized data\n",
        "        # with open(self.output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        #     csvwriter = csv.writer(csvfile)\n",
        "        #     # Write the header\n",
        "        #     csvwriter.writerow(['input_ids', 'target_ids'])\n",
        "\n",
        "        file_exists = os.path.exists(self.output_file)\n",
        "\n",
        "        with open(self.output_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            csvwriter = csv.writer(csvfile)\n",
        "            # Write the header only if the file does not exist\n",
        "            if not file_exists:\n",
        "                csvwriter.writerow(['input_ids', 'target_ids'])\n",
        "                print(f'Output file: creating new: {self.output_file}')\n",
        "            else:\n",
        "              print(f'Output file exists: {self.output_file}')\n",
        "\n",
        "            try:\n",
        "                count = 0\n",
        "                while True:\n",
        "                    # Collect tokens until we have enough for one chunk\n",
        "                    while len(collected_tokens) < self.max_length + 1:\n",
        "                        collected_tokens.append(next(token_gen))\n",
        "\n",
        "                    # Create input and target chunks\n",
        "                    input_tokens = collected_tokens[:self.max_length]\n",
        "                    target_tokens = collected_tokens[1:self.max_length + 1]\n",
        "\n",
        "                    # Save to CSV\n",
        "                    csvwriter.writerow([input_tokens, target_tokens])\n",
        "\n",
        "                    # Preserve tokens for the next chunk\n",
        "                    collected_tokens = collected_tokens[self.stride:]\n",
        "\n",
        "                    count += 1\n",
        "                    if count % 50000==0:\n",
        "                      print(f'count:{count}')\n",
        "            except StopIteration:\n",
        "                # Handle the end of the token generator\n",
        "                print(\"End of file reached.\")\n",
        "                pass\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "    # Testing\n",
        "    # ------------\n",
        "    with open('file1.txt','w') as f:\n",
        "      f.write('zero one two three four \\n five six seven')\n",
        "    with open('file2.txt','w') as f:\n",
        "      f.write('eight nine ten eleven twelve')\n",
        "    tokenizer=TestTokenizer()\n",
        "    # File paths\n",
        "    input_files = ['file1.txt', 'file2.txt']\n",
        "    output_file = 'tokenized_data_test.csv'\n",
        "\n",
        "    # Tokenizer parameters\n",
        "    max_length = 3\n",
        "    stride = 2  # int(max_length*.75) = 384 (using stride as 75% of context length)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for input_file in input_files:\n",
        "        # Tokenize and save to CSV\n",
        "        tokenizer_obj = LargeFileTokenizer(tokenizer, max_length, stride, input_file, output_file)\n",
        "        tokenizer_obj.collect_and_save()\n",
        "    print(f'time taken:{(time.time()-start_time)/60} hours')\n",
        "    print('done!')\n",
        "    # output <!cat tokenized_data_test.csv>\n",
        "    #   input_ids,target_ids\n",
        "    # \"[0, 1, 2]\",\"[1, 2, 3]\"\n",
        "\n",
        "    # five six seven is in new line and we cant see the difference (which is good)\n",
        "    # \"[2, 3, 4]\",\"[3, 4, 5]\"\n",
        "    # \"[4, 5, 6]\",\"[5, 6, 7]\"\n",
        "    # \"[8, 9, 10]\",\"[9, 10, 11]\"\n",
        "\n",
        "    # this is new file so previous stride info. is not saved\n",
        "    # [8, 9, 10]\t[9, 10, 11]\n",
        "\n",
        "    '''\n",
        "    # Initialize tokenizer\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # # Save as tokenizer.json\n",
        "    # tokenizer.save(\"NepaliBPE/tokenizer.json\")\n",
        "    # Load the tokenizer\n",
        "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"NepaliBPE/tokenizer.json\")\n",
        "    # tokenizer.encode('टिकटक वनाउने क्रममा तेह्रथुमको मेन्छ्यायेम गाउँपालिकाको खोरुङगा खोलाको खोंचमा खसेर शिक्षिका र एक छात्राको शनिबार अपराह्न मृत्यु भएको छ ।')\n",
        "\n",
        "    # File paths\n",
        "    input_files = ['nepberta_text.txt', 'iriis_text.txt']\n",
        "    output_file = 'tokenized_data.csv'\n",
        "\n",
        "    # Tokenizer parameters\n",
        "    max_length = 512\n",
        "    stride = 384  # int(max_length*.75) = 384 (using stride as 75% of context length)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for input_file in input_files:\n",
        "        # Tokenize and save to CSV\n",
        "        tokenizer_obj = LargeFileTokenizer(tokenizer, max_length, stride, input_file, output_file)\n",
        "        tokenizer_obj.collect_and_save()\n",
        "    print(f'time taken:{(time.time()-start_time)/60} hours')\n",
        "    print('done!')\n",
        "    '''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Zusiop3rlI"
      },
      "source": [
        "## 2. Train the actual tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6seGXQ2cq8E1",
        "outputId": "610af37f-295d-4a94-8f32-477f14a55036"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "rm: cannot remove 'tokenized_data.csv': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm tokenized_data.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0F74Jl_12eyc",
        "outputId": "428e0ec7-133d-4037-ad4a-5438fed8315d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "file: nepberta_text.txt\n",
            "Output file: creating new: tokenized_data.csv\n",
            "count:50000\n",
            "count:100000\n",
            "count:150000\n",
            "count:200000\n",
            "count:250000\n",
            "count:300000\n",
            "count:350000\n",
            "count:400000\n",
            "count:450000\n",
            "count:500000\n",
            "End of file reached.\n",
            "file: iriis_text.txt\n",
            "Output file exists: tokenized_data.csv\n",
            "count:50000\n",
            "count:100000\n",
            "count:150000\n",
            "count:200000\n",
            "count:250000\n",
            "count:300000\n",
            "count:350000\n",
            "count:400000\n",
            "count:450000\n",
            "count:500000\n",
            "count:550000\n",
            "count:600000\n",
            "count:650000\n",
            "count:700000\n",
            "count:750000\n",
            "count:800000\n",
            "count:850000\n",
            "count:900000\n",
            "count:950000\n",
            "count:1000000\n",
            "count:1050000\n",
            "count:1100000\n",
            "count:1150000\n",
            "count:1200000\n",
            "count:1250000\n",
            "count:1300000\n",
            "count:1350000\n",
            "count:1400000\n",
            "count:1450000\n",
            "count:1500000\n",
            "count:1550000\n",
            "count:1600000\n",
            "count:1650000\n",
            "count:1700000\n",
            "count:1750000\n",
            "count:1800000\n",
            "count:1850000\n",
            "count:1900000\n",
            "count:1950000\n",
            "count:2000000\n",
            "count:2050000\n",
            "count:2100000\n",
            "count:2150000\n",
            "count:2200000\n",
            "count:2250000\n",
            "count:2300000\n",
            "count:2350000\n",
            "count:2400000\n",
            "count:2450000\n",
            "count:2500000\n",
            "count:2550000\n",
            "count:2600000\n",
            "count:2650000\n",
            "count:2700000\n",
            "count:2750000\n",
            "count:2800000\n",
            "count:2850000\n",
            "count:2900000\n",
            "count:2950000\n",
            "count:3000000\n",
            "count:3050000\n",
            "count:3100000\n",
            "count:3150000\n",
            "count:3200000\n",
            "count:3250000\n",
            "count:3300000\n",
            "count:3350000\n",
            "count:3400000\n",
            "count:3450000\n",
            "count:3500000\n",
            "count:3550000\n",
            "count:3600000\n",
            "count:3650000\n",
            "count:3700000\n",
            "count:3750000\n",
            "count:3800000\n",
            "count:3850000\n",
            "count:3900000\n",
            "count:3950000\n",
            "count:4000000\n",
            "count:4050000\n",
            "count:4100000\n",
            "count:4150000\n",
            "count:4200000\n",
            "count:4250000\n",
            "count:4300000\n",
            "count:4350000\n",
            "count:4400000\n",
            "count:4450000\n",
            "count:4500000\n",
            "count:4550000\n",
            "count:4600000\n",
            "count:4650000\n",
            "count:4700000\n",
            "count:4750000\n",
            "End of file reached.\n",
            "time taken:280.25607761144636 hours\n",
            "done!\n"
          ]
        }
      ],
      "source": [
        "import csv\n",
        "import os\n",
        "import time\n",
        "import torch\n",
        "# from transformers import GPT2Tokenizer\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "\n",
        "class LargeFileTokenizer:\n",
        "    def __init__(self, tokenizer, max_length, stride, input_file, output_file):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.stride = stride\n",
        "        self.input_file = input_file\n",
        "        self.output_file = output_file\n",
        "\n",
        "        print(f'file: {self.input_file}')\n",
        "\n",
        "    def tokens_generator(self):\n",
        "        \"\"\"Yields tokenized lines from a large text file.\"\"\"\n",
        "        with open(self.input_file, 'r', encoding='utf-8') as file:\n",
        "            for line in file:\n",
        "                # Tokenize the line and allow special tokens\n",
        "                tokens = self.tokenizer.encode(line.strip())  # , allowed_special={'<|endoftext|>'}\n",
        "                for token in tokens:\n",
        "                    yield token\n",
        "\n",
        "    def collect_and_save(self):\n",
        "        \"\"\"Collects tokens into chunks and saves them to a CSV file.\"\"\"\n",
        "        collected_tokens = []\n",
        "        token_gen = self.tokens_generator()\n",
        "\n",
        "        # # Open the CSV file to save tokenized data\n",
        "        # with open(self.output_file, 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        #     csvwriter = csv.writer(csvfile)\n",
        "        #     # Write the header\n",
        "        #     csvwriter.writerow(['input_ids', 'target_ids'])\n",
        "\n",
        "        file_exists = os.path.exists(self.output_file)\n",
        "\n",
        "        with open(self.output_file, 'a', newline='', encoding='utf-8') as csvfile:\n",
        "            csvwriter = csv.writer(csvfile)\n",
        "            # Write the header only if the file does not exist\n",
        "            if not file_exists:\n",
        "                csvwriter.writerow(['input_ids', 'target_ids'])\n",
        "                print(f'Output file: creating new: {self.output_file}')\n",
        "            else:\n",
        "              print(f'Output file exists: {self.output_file}')\n",
        "\n",
        "            try:\n",
        "                count = 0\n",
        "                while True:\n",
        "                    # Collect tokens until we have enough for one chunk\n",
        "                    while len(collected_tokens) < self.max_length + 1:\n",
        "                        collected_tokens.append(next(token_gen))\n",
        "\n",
        "                    # Create input and target chunks\n",
        "                    input_tokens = collected_tokens[:self.max_length]\n",
        "                    target_tokens = collected_tokens[1:self.max_length + 1]\n",
        "\n",
        "                    # Save to CSV\n",
        "                    csvwriter.writerow([input_tokens, target_tokens])\n",
        "\n",
        "                    # Preserve tokens for the next chunk\n",
        "                    collected_tokens = collected_tokens[self.stride:]\n",
        "\n",
        "                    count += 1\n",
        "                    if count % 50000==0:\n",
        "                      print(f'count:{count}')\n",
        "            except StopIteration:\n",
        "                # Handle the end of the token generator\n",
        "                print(\"End of file reached.\")\n",
        "                pass\n",
        "\n",
        "# Example usage\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    # Initialize tokenizer\n",
        "    # tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # # Save as tokenizer.json\n",
        "    # tokenizer.save(\"NepaliBPE/tokenizer.json\")\n",
        "    # Load the tokenizer\n",
        "    tokenizer = PreTrainedTokenizerFast(tokenizer_file=\"NepaliBPE/tokenizer.json\")\n",
        "    # tokenizer.encode('टिकटक वनाउने क्रममा तेह्रथुमको मेन्छ्यायेम गाउँपालिकाको खोरुङगा खोलाको खोंचमा खसेर शिक्षिका र एक छात्राको शनिबार अपराह्न मृत्यु भएको छ ।')\n",
        "\n",
        "    # File paths\n",
        "    input_files = ['nepberta_text.txt', 'iriis_text.txt']\n",
        "    output_file = 'tokenized_data.csv'\n",
        "\n",
        "    # Tokenizer parameters\n",
        "    max_length = 512\n",
        "    stride = 384  # int(max_length*.75) = 384 (using stride as 75% of context length)\n",
        "\n",
        "    start_time = time.time()\n",
        "    for input_file in input_files:\n",
        "        # Tokenize and save to CSV\n",
        "        tokenizer_obj = LargeFileTokenizer(tokenizer, max_length, stride, input_file, output_file)\n",
        "        tokenizer_obj.collect_and_save()\n",
        "    print(f'time taken:{(time.time()-start_time)/60} hours')\n",
        "    print('done!')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "mDA3AsKN1feM",
        "K9Zusiop3rlI"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}