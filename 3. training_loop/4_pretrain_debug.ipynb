{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zu7SXf76pnaC",
        "outputId": "965368ce-3e9d-4014-8ce4-1e7078a80021"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Research/llama3\n",
            "3_pretrain.ipynb\t\tfunctions.py\t   previous_chapters.py\n",
            "3_pretrain.py\t\t\thf_cache\t   __pycache__\n",
            "cleaned_bhagavad_gita_data.txt\tllama_debug_model  standalone_llama32_bhagvada_gita_trainer.ipynb\n",
            "debug_dataloaders.py\t\tmodel_checkpoints  tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "%cd /content/drive/MyDrive/Research/llama3\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UFXD73X3pfpH",
        "outputId": "bf1495a9-be82-4d3f-d4f8-dd3d430d720a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting blobfile>=3.0.0 (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1))\n",
            "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: huggingface_hub>=0.24.7 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (0.27.0)\n",
            "Collecting ipywidgets>=8.1.2 (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3))\n",
            "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
            "Requirement already satisfied: safetensors>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 4)) (0.4.5)\n",
            "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 5)) (0.2.0)\n",
            "Collecting pycryptodomex>=3.8 (from blobfile>=3.0.0->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1))\n",
            "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile>=3.0.0->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1)) (2.2.3)\n",
            "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile>=3.0.0->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1)) (5.3.0)\n",
            "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile>=3.0.0->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1)) (3.16.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (2024.10.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (4.12.2)\n",
            "Collecting comm>=0.1.3 (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3))\n",
            "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
            "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (7.34.0)\n",
            "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (5.7.1)\n",
            "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3))\n",
            "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (3.0.13)\n",
            "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (75.1.0)\n",
            "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3))\n",
            "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (4.4.2)\n",
            "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.7.5)\n",
            "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (3.0.48)\n",
            "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (2.18.0)\n",
            "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.2.0)\n",
            "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.1.7)\n",
            "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (4.9.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (2024.12.14)\n",
            "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.8.4)\n",
            "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.7.0)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.2.13)\n",
            "Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
            "Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: widgetsnbextension, pycryptodomex, jedi, comm, blobfile, ipywidgets\n",
            "  Attempting uninstall: widgetsnbextension\n",
            "    Found existing installation: widgetsnbextension 3.6.10\n",
            "    Uninstalling widgetsnbextension-3.6.10:\n",
            "      Successfully uninstalled widgetsnbextension-3.6.10\n",
            "  Attempting uninstall: ipywidgets\n",
            "    Found existing installation: ipywidgets 7.7.1\n",
            "    Uninstalling ipywidgets-7.7.1:\n",
            "      Successfully uninstalled ipywidgets-7.7.1\n",
            "Successfully installed blobfile-3.0.0 comm-0.2.2 ipywidgets-8.1.5 jedi-0.19.2 pycryptodomex-3.21.0 widgetsnbextension-4.0.13\n"
          ]
        }
      ],
      "source": [
        "!pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23JMDjMtqGyr",
        "outputId": "7d45347a-6be6-4dbc-f69b-1be93698c599"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install blobfile datasets --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UqxQ62Iqp-xk",
        "outputId": "b771858d-358a-46b0-b5a0-159c1f724b56"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "blobfile version: 3.0.0\n",
            "huggingface_hub version: 0.27.0\n",
            "torch version: 2.5.1+cu121\n"
          ]
        }
      ],
      "source": [
        "from importlib.metadata import version\n",
        "\n",
        "pkgs = [\n",
        "    \"blobfile\",         # to download pretrained weights\n",
        "    \"huggingface_hub\",  # to download pretrained weights\n",
        "    # \"tiktoken\",         # to implement the tokenizer\n",
        "    \"torch\",            # to implement the model\n",
        "]\n",
        "for p in pkgs:\n",
        "    print(f\"{p} version: {version(p)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aPMYqSeKuIpM",
        "outputId": "e7f94d8b-ae9c-486c-ca09-c4bea39efee4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting 3_pretrain.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile 3_pretrain.py\n",
        "# code from https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb\n",
        "\n",
        "import argparse\n",
        "import math\n",
        "import os\n",
        "from pathlib import Path\n",
        "import time\n",
        "\n",
        "\n",
        "# modified. tokenizer import\n",
        "# import tiktoken\n",
        "from transformers import PreTrainedTokenizerFast\n",
        "import torch\n",
        "from previous_chapters import (\n",
        "    # create_dataloader_v2, # modified. use create_dataloader_v3 instead\n",
        "    create_dataloader_v3,\n",
        "    Llama3Model,\n",
        "    generate_and_print_sample,\n",
        "    calc_loss_batch,\n",
        "    evaluate_model,\n",
        "    plot_losses,\n",
        "    Tokenizer,\n",
        "    ChatFormat\n",
        ")\n",
        "\n",
        "from functions import delete_checkpoints_except_n_highest_steps, get_max_global_step_file\n",
        "from debug_dataloaders import create_debug_dataloaders\n",
        "\n",
        "\n",
        "def create_dataloaders(num_workers=0):\n",
        "    '''\n",
        "    modified. sebastian\n",
        "    parameter: text_data is removed\n",
        "    parameter: max_length, stride are removed\n",
        "\n",
        "    modified. GPT2\n",
        "    parameter: batch_size: removed  (since we are loading pre tokenized data from huggingface)\n",
        "    parameter: train_ratio: removed (data is pre-split in train and test in hf data (since it takes a while to split))\n",
        "    '''\n",
        "    train_loader, val_loader = create_dataloader_v3(\n",
        "        shuffle=False,  # modified. to avoid  shuffling the data\n",
        "        drop_last=True,\n",
        "        num_workers=num_workers,\n",
        "        # context_length=args.context_length\n",
        "    )\n",
        "    return train_loader, val_loader\n",
        "\n",
        "\n",
        "def convert_time(seconds):\n",
        "    hours, rem = divmod(seconds, 3600)\n",
        "    minutes, seconds = divmod(rem, 60)\n",
        "    return int(hours), int(minutes), int(seconds)\n",
        "\n",
        "\n",
        "\n",
        "BOOK_VERSION = True\n",
        "\n",
        "\n",
        "\n",
        "def train_model(model, train_loader, val_loader, optimizer, device,\n",
        "        n_epochs, eval_freq, eval_iter, start_context, output_dir, tokenizer,\n",
        "        warmup_steps, previous_global_step=None, initial_lr=3e-05, min_lr=1e-6,\n",
        "        train_losses = [], val_losses=[], track_tokens_seen=[], track_lrs=[],\n",
        "        previous_epochs = 0\n",
        "            ):\n",
        "    print(\"Training ...\")\n",
        "    # train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
        "    tokens_seen, global_step = 0, -1\n",
        "\n",
        "    # modified. for resuming\n",
        "    train_loader_index = -1\n",
        "    train_loader_resume_index = previous_global_step % len_train_loader if previous_global_step else -1\n",
        "\n",
        "    # Retrieve the maximum learning rate from the optimizer\n",
        "    peak_lr = optimizer.param_groups[0][\"lr\"]\n",
        "\n",
        "    # Calculate the total number of iterations in the training process\n",
        "    total_training_steps = len_train_loader * n_epochs# len(train_loader) * n_epochs\n",
        "\n",
        "    # Calculate the learning rate increment during the warmup phase\n",
        "    lr_increment = (peak_lr - initial_lr) / warmup_steps\n",
        "    try:\n",
        "        done_resume = False # modified. to check if the resume script has been run once\n",
        "        for epoch in range(n_epochs):\n",
        "            model.train()   # Training mode\n",
        "            for input_batch, target_batch in train_loader:\n",
        "                optimizer.zero_grad()\n",
        "                global_step += 1\n",
        "\n",
        "                if args.debug:\n",
        "                    # stop early for debugging\n",
        "                    # if global_step > 10000:\n",
        "                    #     print(\"Debugging: stopping early\")\n",
        "                    #     generate_and_print_sample(PROMPT=\"रामले भात\", tokenizer=tokenizer, chat_tokenizer=chat_tokenizer, model=model, device=device, context_length = LLAMA32_CONFIG[\"context_length\"])\n",
        "                    #     break\n",
        "                    track_lrs = []\n",
        "                else:\n",
        "                    # modified. added to resume feature\n",
        "                    if not done_resume and previous_global_step and train_loader_index < train_loader_resume_index:\n",
        "                        # naive implementation.\n",
        "                        # to iterate through train_loader until train_loader_index gets to train_loader_resume_index\n",
        "\n",
        "                        train_loader_index += 1    # previous_global_step % len(train_loader)\n",
        "                        # print('.', end = '')\n",
        "                        continue    # continue train_loader till global_step gets to previous_global_step\n",
        "                    # modified. added\n",
        "                    if not done_resume and previous_global_step:\n",
        "                        # this code is supposed to runs only once\n",
        "                        done_resume = True\n",
        "                        global_step = previous_global_step\n",
        "                        print('\\n' + '-'*70 + '\\n')\n",
        "                        print(f\"\\n{'-'*70}\\n resuming from global_step : {global_step} \\n train_loader_index: {train_loader_index} \\n len_train_loader: {len_train_loader}\", end = '\\n' + '-'*70 + '\\n')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "                    # Adjust the learning rate based on the current phase (warmup or cosine annealing)\n",
        "                    if global_step < warmup_steps:\n",
        "                        # Linear warmup\n",
        "                        lr = initial_lr + global_step * lr_increment\n",
        "                    else:\n",
        "                        # Cosine annealing after warmup\n",
        "                        progress = ((global_step - warmup_steps) /\n",
        "                                    (total_training_steps - warmup_steps))\n",
        "                        lr = min_lr + (peak_lr - min_lr) * 0.5 * (1 + math.cos(math.pi * progress))\n",
        "\n",
        "                    # Apply the calculated learning rate to the optimizer\n",
        "                    for param_group in optimizer.param_groups:\n",
        "                        param_group[\"lr\"] = lr\n",
        "                    track_lrs.append(lr)  # Store the current learning rate\n",
        "\n",
        "                # Calculate and backpropagate the loss\n",
        "                loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
        "                loss.backward()\n",
        "\n",
        "                # Apply gradient clipping after the warmup phase to avoid exploding gradients\n",
        "\n",
        "\n",
        "                if not args.debug:\n",
        "                    '''\n",
        "                    * Gradient clipping might be unnecessary during this warm-up because gradients tend to be smaller.\n",
        "                    '''\n",
        "                    if BOOK_VERSION:\n",
        "                        if global_step > warmup_steps:\n",
        "                            # Triggered After completing the warm-up phase (dont know why this matters. it was implemented by sebastian)\n",
        "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                    else:\n",
        "                        if global_step >= warmup_steps:  # the book originally used global_step > warmup_steps, which lead to a skipped clipping step after warmup\n",
        "                            # Triggered During and after the last warm-up step\n",
        "                            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "                optimizer.step()\n",
        "                tokens_seen += input_batch.numel()\n",
        "\n",
        "                # Periodically evaluate the model on the training and validation sets\n",
        "\n",
        "                if global_step % eval_freq == 0:\n",
        "                    train_loss, val_loss = evaluate_model(\n",
        "                        model, train_loader, val_loader,\n",
        "                        device, eval_iter, len_train_loader, len_val_loader\n",
        "                    )\n",
        "                    train_losses.append(train_loss)\n",
        "                    val_losses.append(val_loss)\n",
        "                    track_tokens_seen.append(tokens_seen)\n",
        "                    # Print the current losses\n",
        "                    print(f\"Ep {epoch+1} (Iter {global_step:06d}): \"\n",
        "                        f\"Train loss {train_loss:.3f}, \"\n",
        "                        f\"Val loss {val_loss:.3f}\"\n",
        "                    )\n",
        "\n",
        "                # Save at every 10,000 steps\n",
        "                if global_step % args.save_ckpt_freq_steps == 0 and global_step != 0:\n",
        "                    delete_checkpoints_except_n_highest_steps(n=1)  # modified. to delete the previous steps checkpoint#\n",
        "                    save_file_path = os.path.join(output_dir, f\"model_pg_{global_step}_steps.pth\")\n",
        "                    torch.save({\n",
        "                        \"model_state_dict\": model.state_dict(),\n",
        "                        \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                        \"train_losses\": train_losses,\n",
        "                        \"val_losses\": val_losses,\n",
        "                        \"track_tokens_seen\": track_tokens_seen,\n",
        "                        \"track_lrs\": track_lrs,\n",
        "                        \"epochs\": global_step % len_train_loader if global_step > len_train_loader else 0,\n",
        "                        \"global_step\": global_step +1,  # +1 because next `global_step` will be incremented by 1 and we will set: next `global_step = previous_global_step``\n",
        "                        },\n",
        "                        save_file_path\n",
        "                    )\n",
        "                    print(f\"Saved {save_file_path}\")\n",
        "                    # Generate and print a sample from the model to monitor progress (at the end of each epoch)\n",
        "                    generate_and_print_sample(PROMPT=\"रामले भात\", tokenizer=tokenizer, chat_tokenizer=chat_tokenizer, model=model, device=device, context_length = LLAMA32_CONFIG[\"context_length\"])\n",
        "                    # generate_and_print_sample(\n",
        "                    #     model, tokenizer, device, start_context\n",
        "                    # )\n",
        "\n",
        "            # Save at the end of each epoch\n",
        "            delete_checkpoints_except_n_highest_steps(n=1)  # modified. to delete the previous steps checkpoint\n",
        "            new_epochs = global_step % len_train_loader if global_step > len_train_loader else 0\n",
        "            save_file_path = os.path.join(output_dir, f\"model_pg_epoch_{new_epochs}.pth\")\n",
        "            torch.save({\n",
        "                    \"model_state_dict\": model.state_dict(),\n",
        "                    \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "                    \"train_losses\": train_losses,\n",
        "                    \"val_losses\": val_losses,\n",
        "                    \"track_tokens_seen\": track_tokens_seen,\n",
        "                    \"track_lrs\": track_lrs,\n",
        "                    \"epochs\": new_epochs,\n",
        "                    \"global_step\": global_step +1,  # +1 because next `global_step` will be incremented by 1 and we will set: next `global_step = previous_global_step``\n",
        "                    },\n",
        "                    save_file_path\n",
        "            )\n",
        "            print(f\"Saved {save_file_path}\")\n",
        "    except KeyboardInterrupt:\n",
        "        file_name = os.path.join(output_dir, f\"model_pg_{global_step}_interrupted.pth\")\n",
        "        # modified. to save optimizer state_dict along with model state dict\n",
        "        # torch.save(model.state_dict(), file_name)\n",
        "        torch.save({\n",
        "            \"model_state_dict\": model.state_dict(),\n",
        "            \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "            \"train_losses\": train_losses,\n",
        "            \"val_losses\": val_losses,\n",
        "            \"track_tokens_seen\": track_tokens_seen,\n",
        "            \"track_lrs\": track_lrs,\n",
        "            \"epochs\": global_step % len_train_loader if global_step > len_train_loader else 0,\n",
        "            \"global_step\": global_step,\n",
        "            },\n",
        "            file_name\n",
        "        )\n",
        "        print(f\"Saved {file_name}\")\n",
        "\n",
        "    return train_losses, val_losses, track_tokens_seen, track_lrs\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Note:\n",
        "    # Uncomment the following code to calculate the execution time\n",
        "    start_time = time.time()\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='LLAMA3.2 Model Training Configuration')\n",
        "\n",
        "\n",
        "    parser.add_argument('--output_dir', type=str, default='model_checkpoints',\n",
        "                        help='Directory where the model checkpoints will be saved')\n",
        "    parser.add_argument('--n_epochs', type=int, default=1,\n",
        "                        help='Number of epochs to train the model')\n",
        "    parser.add_argument('--print_sample_iter', type=int, default=1000,\n",
        "                        help='Iterations between printing sample outputs')\n",
        "    parser.add_argument('--eval_freq', type=int, default=100,\n",
        "                        help='Frequency of evaluations during training')\n",
        "    parser.add_argument('--save_ckpt_freq', type=int, default=100_000,\n",
        "                        help='Frequency of saving model checkpoints during training')\n",
        "    parser.add_argument('--lr', type=float, default=0.001,\n",
        "                        help='Learning rate for the optimizer') # this was originally set to 5e-4 in the book by mistake\n",
        "    parser.add_argument('--batch_size', type=int, default=4,\n",
        "                        help='Batch size for training')\n",
        "    parser.add_argument('--debug', type=bool, default=False,\n",
        "                        help='Uses a very small model for debugging purposes')\n",
        "    parser.add_argument('--max_text_len', type=int, default=45000000,\n",
        "                        help='testing different text sizes.')\n",
        "\n",
        "    # modified. added resume_from_previous_training\n",
        "    parser.add_argument('--resume_from_previous_training', type=str, default=\"True\",\n",
        "                        help='whether or not to resume from saved previous training checkpoint')\n",
        "    parser.add_argument('--push_to_hub_every_n_hours', type=int, default=6,\n",
        "                        help='how often to push to hub in hours.')\n",
        "    parser.add_argument('--save_ckpt_freq_steps', type=int, default=10_000,\n",
        "                        help='how often to save the model checkpoint in steps')\n",
        "    parser.add_argument('--context_length', type=int, default=1024,\n",
        "                        help='context length (default: 1024)')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "    args.resume_from_previous_training = args.resume_from_previous_training.lower() == 'true'\n",
        "    torch.manual_seed(123)\n",
        "\n",
        "\n",
        "    # modified. code to load the tokenizer\n",
        "    # tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
        "    # tokenizer = PreTrainedTokenizerFast.from_pretrained(\"Aananda-giri/NepaliBPE\")\n",
        "    tokenizer = Tokenizer(\"tokenizer.json\")\n",
        "    chat_tokenizer = ChatFormat(tokenizer)\n",
        "\n",
        "    if args.debug:\n",
        "        print(f'---------------------\\nDEBUG MODE\\n---------------------')\n",
        "        # Debug mode\n",
        "        LLAMA32_CONFIG = {\n",
        "            # d_out = emb_dim\n",
        "            # Embedding dimension <d_out // num_heads> must be even\n",
        "            \"vocab_size\": 50006,      # <len(tokenizer.tokenizer)=50006> Vocabulary size\n",
        "            \"context_length\": 10,  # Context length\n",
        "            # d_in=d_out=emb_dim,\n",
        "            # d_out must be divisible by num_heads\n",
        "            \"emb_dim\": 8,            # Embedding dimension\n",
        "            # (num_heads must be divisible by num_kv_groups)\n",
        "            \"n_heads\": 4,              # Number of attention heads\n",
        "            \"n_layers\": 2,             # Number of layers\n",
        "            \"hidden_dim\": 16,         # Size of the intermediate dimension in FeedForward\n",
        "            \"n_kv_groups\": 2,           # Key-Value groups for grouped-query attention\n",
        "            \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
        "            \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
        "            \"rope_freq\": {              # RoPE frequency scaling\n",
        "                \"factor\": 32.0,\n",
        "                \"low_freq_factor\": 1.0,\n",
        "                \"high_freq_factor\": 4.0,\n",
        "                \"original_context_length\": 8192,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Custom dataloader for debug mode\n",
        "        # --------------------------------\n",
        "        def read_text_file(file_path):\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "                text_data = file.read()\n",
        "            return text_data\n",
        "        text_data = read_text_file(\"cleaned_bhagavad_gita_data.txt\") + \" <|endoftext|> \"\n",
        "        train_loader, val_loader = create_debug_dataloaders(\n",
        "            text_data,\n",
        "            tokenizer,\n",
        "            train_ratio=0.9,\n",
        "            batch_size=2,\n",
        "            max_length=LLAMA32_CONFIG[\"context_length\"],\n",
        "            stride=LLAMA32_CONFIG[\"context_length\"],\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "    else:\n",
        "        # Llama 3.2 200M\n",
        "        LLAMA32_CONFIG = {\n",
        "            \"vocab_size\": 50006,       # <len(tokenizer.tokenizer)=50006> 128_256 reduced vocabulary size\n",
        "            \"context_length\": 512,      # 131_072 reduced Context length (unrelated to model size but higheer context length consumes more RAM)\n",
        "            \"emb_dim\": 1024,            # 2048 reduced Embedding dimension\n",
        "            \"n_heads\": 16,              # 32 reduced Number of attention heads\n",
        "            \"n_layers\": 8,             # 16 reduced Number of layers\n",
        "            \"hidden_dim\": 4096,         # 8192 Size of the intermediate dimension in FeedForward\n",
        "            \"n_kv_groups\": 8,           # 8 Key-Value groups for grouped-query attention\n",
        "            \"rope_base\": 500_000.0,     # 500_000 The base in RoPE's \"theta\"\n",
        "            \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
        "            \"rope_freq\": {              # RoPE frequency scaling\n",
        "                \"factor\": 32.0,\n",
        "                \"low_freq_factor\": 1.0,\n",
        "                \"high_freq_factor\": 4.0,\n",
        "                \"original_context_length\": 8192,\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Initialize new data loader\n",
        "        train_loader, val_loader = create_dataloaders(\n",
        "            # train_ratio=0.9,\n",
        "            # batch_size=args.batch_size,\n",
        "            num_workers=0\n",
        "        )\n",
        "\n",
        "    LLAMA_SIZE_STR = \"2M\" if args.debug else \"200M\"\n",
        "\n",
        "    def set_loader_lengths(debug, train_loader=None, val_loader=None):\n",
        "        if debug:\n",
        "            len_train_loader = len(train_loader)\n",
        "            len_val_loader = len(val_loader)\n",
        "        else:\n",
        "            len_train_loader = 4781060\n",
        "            len_val_loader = 531229\n",
        "        return len_train_loader, len_val_loader\n",
        "\n",
        "    global len_train_loader\n",
        "    global len_val_loader\n",
        "\n",
        "    len_train_loader, len_val_loader = set_loader_lengths(\n",
        "        args.debug, train_loader, val_loader\n",
        "    )\n",
        "    # re-scaling theta\n",
        "    # ------------------------------------------------------------\n",
        "\n",
        "    old_context_length = 131_072    # original context length of llama3.2 model\n",
        "    new_context_length = LLAMA32_CONFIG[\"context_length\"]  # 512 our new context length\n",
        "\n",
        "    def rescale_theta(theta_old, context_length_old, context_length_new):\n",
        "        # # linear scaling by sebastian\n",
        "        # scaling_factor = context_length_new / context_length_old\n",
        "\n",
        "        '''\n",
        "            Using square root scaling (instead of linear scaling as done by sebastian),\n",
        "            because linear scaling is resulting in very small theta value.\n",
        "            which is slowing the training (slower decrease in loss)\n",
        "            might be because of the large difference in context length (137_072 vs 512)\n",
        "        '''\n",
        "        scaling_factor = math.sqrt(context_length_new/context_length_old)\n",
        "        theta_new = theta_old * scaling_factor\n",
        "        return theta_new\n",
        "\n",
        "    LLAMA32_CONFIG[\"rope_base\"] = rescale_theta(\n",
        "        LLAMA32_CONFIG[\"rope_base\"],\n",
        "        old_context_length,\n",
        "        new_context_length\n",
        "    )\n",
        "\n",
        "    print(\"New RoPE theta (i.e. LLAMA32_CONFIG[\\\"rope_base\\\"]):\", LLAMA32_CONFIG[\"rope_base\"])\n",
        "\n",
        "\n",
        "    model = Llama3Model(LLAMA32_CONFIG)\n",
        "\n",
        "    # Check buffers\n",
        "    # --------------\n",
        "    print('The following is expected to print True to confirm buffers are reused instead of being (wastefully) recreated:')\n",
        "    print(model.trf_blocks[0].att.mask is model.trf_blocks[-1].att.mask)\n",
        "    print(model.trf_blocks[0].att.cos is model.trf_blocks[-1].att.cos)\n",
        "    print(model.trf_blocks[0].att.sin is model.trf_blocks[-1].att.sin)\n",
        "\n",
        "    # Display number of parameters\n",
        "    # -----------------------------\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total number of parameters: {total_params:,}\")\n",
        "    # Account for weight tying\n",
        "    total_params_normalized = total_params - model.tok_emb.weight.numel()\n",
        "    print(f\"\\nTotal number of unique parameters: {total_params_normalized:,}\")\n",
        "\n",
        "    # Display model_memory_size\n",
        "    # -----------------------------------------------------------------------\n",
        "    def model_memory_size(model, input_dtype=torch.float32):\n",
        "        total_params = 0\n",
        "        total_grads = 0\n",
        "        for param in model.parameters():\n",
        "            # Calculate total number of elements per parameter\n",
        "            param_size = param.numel()\n",
        "            total_params += param_size\n",
        "            # Check if gradients are stored for this parameter\n",
        "            if param.requires_grad:\n",
        "                total_grads += param_size\n",
        "\n",
        "        # Calculate buffer size (non-parameters that require memory)\n",
        "        total_buffers = sum(buf.numel() for buf in model.buffers())\n",
        "\n",
        "        # Size in bytes = (Number of elements) * (Size of each element in bytes)\n",
        "        # We assume parameters and gradients are stored in the same type as input dtype\n",
        "        element_size = torch.tensor(0, dtype=input_dtype).element_size()\n",
        "        total_memory_bytes = (total_params + total_grads + total_buffers) * element_size\n",
        "\n",
        "        # Convert bytes to gigabytes\n",
        "        total_memory_gb = total_memory_bytes / (1024**3)\n",
        "\n",
        "        return total_memory_gb\n",
        "\n",
        "    print(f\"float32 (PyTorch default): {model_memory_size(model, input_dtype=torch.float32):.2f} GB\")\n",
        "    print(f\"bfloat16: {model_memory_size(model, input_dtype=torch.bfloat16):.2f} GB\")\n",
        "    # -----------------------------------------------------------------------\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        device = torch.device(\"cuda\")\n",
        "    elif torch.backends.mps.is_available():\n",
        "        device = torch.device(\"mps\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "\n",
        "    model.to(device)\n",
        "    print(f'device: {device}')\n",
        "\n",
        "\n",
        "\n",
        "    # model = GPTModel(GPT_CONFIG_124M)\n",
        "    # model.to(device)\n",
        "    peak_lr = args.lr # 0.001  # this was originally set to 5e-4 in the book by mistake\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=peak_lr, weight_decay=0.1)  # the book accidentally omitted the lr assignment\n",
        "\n",
        "    output_dir = Path(args.output_dir)\n",
        "    output_dir.mkdir(parents=True, exist_ok=True)\n",
        "    # global_step=0\n",
        "\n",
        "    train_losses, val_losses, track_tokens_seen, track_lrs = [], [], [], []\n",
        "    # previous_epochs = 0\n",
        "    previous_global_step = None\n",
        "    # this should work for epochs but epochs take a long time to train (so were sabing for every 10,000 steps)\n",
        "    # latest_model_checkpoint = get_max_epoch_file(directory='model_checkpoints')\n",
        "    latest_model_checkpoint = get_max_global_step_file(directory='model_checkpoints')\n",
        "\n",
        "    # if args.load_model and os.path.exists(output_dir):\n",
        "    print(f'\\n\\nargs.resume_from_previous_training: {args.resume_from_previous_training}\\n\\n')\n",
        "    if latest_model_checkpoint and args.resume_from_previous_training:\n",
        "\n",
        "        print(f'Loading existing model: {latest_model_checkpoint}', end = '\\n' + '-'*70 + '\\n')\n",
        "\n",
        "        checkpoint = torch.load(latest_model_checkpoint, weights_only=False)\n",
        "\n",
        "        # modified (added model loading code)\n",
        "        model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
        "\n",
        "        optimizer = torch.optim.AdamW(model.parameters(), lr=peak_lr, weight_decay=0.1)  # the book accidentally omitted the lr assignment\n",
        "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
        "\n",
        "        train_losses = checkpoint[\"train_losses\"]\n",
        "        print(f'train_losses: {type(train_losses)}  len: {len(train_losses)}')\n",
        "\n",
        "        val_losses = checkpoint[\"val_losses\"]\n",
        "        print(f'val_losses: {type(val_losses)}  len: {len(val_losses)}')\n",
        "\n",
        "        track_tokens_seen = checkpoint[\"track_tokens_seen\"]\n",
        "        print(f'track_tokens_seen: {type(track_tokens_seen)}  len: {len(track_tokens_seen)}')\n",
        "\n",
        "        track_lrs = checkpoint[\"track_lrs\"]\n",
        "        print(f'track_lrs: {type(track_lrs)}  len: {len(track_lrs)}')\n",
        "\n",
        "        previous_epochs = checkpoint[\"epochs\"]\n",
        "        print(f'previous epochs: {type(previous_epochs)} {previous_epochs}')\n",
        "\n",
        "        previous_global_step = checkpoint[\"global_step\"]\n",
        "        print(f'previous global step: {previous_global_step} \\n previous epochs: {previous_epochs}')\n",
        "        print(end = '\\n' + '-'*70 + '\\n')\n",
        "\n",
        "    else:\n",
        "        print(f'starting new model from scratch')\n",
        "\n",
        "    # modified\n",
        "    # n_epochs = 15\n",
        "    n_epochs = args.n_epochs\n",
        "\n",
        "    # data_dir = args.data_dir\n",
        "    # all_files = [os.path.join(path, name) for path, subdirs, files\n",
        "    #              in os.walk(data_dir) for name in files if name.endswith((\".txt\"))]\n",
        "    # total_files = len(all_files)\n",
        "\n",
        "    # if total_files == 0:\n",
        "    #     print(\"No training text files found. Make sure you \"\n",
        "    #           \"selected the correct input directory\")\n",
        "    #     quit()\n",
        "    # print(\"Total files:\", total_files)\n",
        "\n",
        "    # for index, file_path in enumerate(all_files, 1):\n",
        "    # book_start_time = time.time()\n",
        "    # text_data = read_text_file(file_path) + \" <|endoftext|> \"\n",
        "    # text_data = text_data[:args.max_text_len]\n",
        "    # print(f\"Tokenizing file {index} of {total_files}: {file_path}\")\n",
        "\n",
        "    print(f'len. train_loader: {len_train_loader}')\n",
        "    print(f'len.val_loader: {len_val_loader}')  # len(val_loader)\n",
        "\n",
        "    total_steps = len_train_loader * n_epochs\n",
        "    warmup_steps = int(0.2 * total_steps) # 20% warmup\n",
        "    print(f' warmup_steps: {warmup_steps}')\n",
        "\n",
        "    train_losses, val_losses, track_tokens_seen, track_lrs = train_model(\n",
        "        model, train_loader, val_loader, optimizer, device, n_epochs=n_epochs,\n",
        "        eval_freq=args.eval_freq, eval_iter=1, start_context=\"रामले भात\", # \"Every effort moves you\", <modified>\n",
        "        output_dir=output_dir, tokenizer=tokenizer, warmup_steps=warmup_steps, previous_global_step=previous_global_step,\n",
        "        initial_lr=1e-5, min_lr=1e-5,\n",
        "        train_losses = train_losses, val_losses=val_losses, track_tokens_seen=track_tokens_seen, track_lrs=track_lrs,\n",
        "        # previous_epochs = previous_epochs\n",
        "\n",
        "    )\n",
        "    epochs_tensor = torch.linspace(0, args.n_epochs, len(train_losses))\n",
        "    plot_losses(epochs_tensor, track_tokens_seen, train_losses, val_losses, output_dir)\n",
        "\n",
        "    # print_eta(start_time, book_start_time, index, total_files)\n",
        "\n",
        "\n",
        "    # modified. to save optimizer state_dict along with model state dict\n",
        "    # torch.save(model.state_dict(), output_dir / \"model_pg_final.pth\")\n",
        "\n",
        "    # lets save at the end of each epoch instead\n",
        "    # torch.save({\n",
        "    #     \"model_state_dict\": model.state_dict(),\n",
        "    #     \"optimizer_state_dict\": optimizer.state_dict(),\n",
        "    #     \"train_losses\": train_losses,\n",
        "    #     \"train_losses\": train_losses,\n",
        "    #     \"track_tokens_seen\": track_tokens_seen,\n",
        "    #     \"track_lrs\": track_lrs,\n",
        "    #     \"epochs\": n_epochs + previous_epochs,\n",
        "    #     },\n",
        "    #     output_dir / \"model_pg_final.pth\"\n",
        "    # )\n",
        "    print(f\"Maximum GPU memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")\n",
        "\n",
        "    # Note:\n",
        "    # Uncomment the following code to show the execution time\n",
        "    end_time = time.time()\n",
        "    execution_time_minutes = (end_time - start_time) / 60\n",
        "    print(f\"Training completed in {execution_time_minutes:.2f} minutes.\")# code from https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJbmfOqO4K8q",
        "outputId": "6bec0ec4-1c3f-4b7f-d5d3-5e681c373366"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting previous_chapters.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile previous_chapters.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZB4TjOvCbIr",
        "outputId": "ff6ca4cc-ac66-416a-9a94-7fd8a929a318"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting debug_dataloaders.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile debug_dataloaders.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WzdzMhL7GmrS",
        "outputId": "803452e3-753e-48d9-87f0-7b1383ea2bc7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3_pretrain.ipynb\t\tfunctions.py\t   previous_chapters.py\n",
            "3_pretrain.py\t\t\thf_cache\t   __pycache__\n",
            "cleaned_bhagavad_gita_data.txt\tllama_debug_model  standalone_llama32_bhagvada_gita_trainer.ipynb\n",
            "debug_dataloaders.py\t\tmodel_checkpoints  tokenizer.json\n"
          ]
        }
      ],
      "source": [
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python 3_pretrain.py \\\n",
        " --resume_from_previous_training False\\\n",
        " --n_epochs 3 \\\n",
        " --print_sample_iter 2_000 \\\n",
        " --save_ckpt_freq 2_000 \\\n",
        " --batch_size 2 \\\n",
        " --eval_freq 300 \\\n",
        " --output_dir model_checkpoints \\\n",
        " --save_ckpt_freq_steps 100000 \\\n",
        " --debug True \\\n",
        " --lr 5e-4 \\\n",
        "\n",
        "#  n_epochs"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWweXfduhQr9",
        "outputId": "44e29d5a-69a9-4dd9-d9f6-78ea54a93135"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------\n",
            "DEBUG MODE\n",
            "---------------------\n",
            "New RoPE theta (i.e. LLAMA32_CONFIG[\"rope_base\"]): 4367.320268554277\n",
            "The following is expected to print True to confirm buffers are reused instead of being (wastefully) recreated:\n",
            "True\n",
            "True\n",
            "True\n",
            "Total number of parameters: 801,288\n",
            "\n",
            "Total number of unique parameters: 401,240\n",
            "float32 (PyTorch default): 0.01 GB\n",
            "bfloat16: 0.00 GB\n",
            "device: cuda\n",
            "\n",
            "\n",
            "args.resume_from_previous_training: False\n",
            "\n",
            "\n",
            "starting new model from scratch\n",
            "len. train_loader: 13844\n",
            "len.val_loader: 1581\n",
            " warmup_steps: 8306\n",
            "Training ...\n",
            "Ep 1 (Iter 000000): Train loss 10.750, Val loss 10.938\n",
            "Ep 1 (Iter 000300): Train loss 10.875, Val loss 10.875\n",
            "Ep 1 (Iter 000600): Train loss 10.875, Val loss 10.875\n",
            "Ep 1 (Iter 000900): Train loss 10.938, Val loss 10.812\n",
            "Ep 1 (Iter 001200): Train loss 10.938, Val loss 10.812\n",
            "Ep 1 (Iter 001500): Train loss 10.875, Val loss 10.812\n",
            "Ep 1 (Iter 001800): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 002100): Train loss 10.938, Val loss 10.812\n",
            "Ep 1 (Iter 002400): Train loss 10.875, Val loss 10.812\n",
            "Ep 1 (Iter 002700): Train loss 10.750, Val loss 10.812\n",
            "Ep 1 (Iter 003000): Train loss 10.875, Val loss 10.812\n",
            "Ep 1 (Iter 003300): Train loss 10.875, Val loss 10.812\n",
            "Ep 1 (Iter 003600): Train loss 10.875, Val loss 10.812\n",
            "Ep 1 (Iter 003900): Train loss 10.688, Val loss 10.812\n",
            "Ep 1 (Iter 004200): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 004500): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 004800): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 005100): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 005400): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 005700): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 006000): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 006300): Train loss 10.750, Val loss 10.812\n",
            "Ep 1 (Iter 006600): Train loss 10.750, Val loss 10.812\n",
            "Ep 1 (Iter 006900): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 007200): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 007500): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 007800): Train loss 10.750, Val loss 10.812\n",
            "Ep 1 (Iter 008100): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 008400): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 008700): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 009000): Train loss 10.812, Val loss 10.812\n",
            "Ep 1 (Iter 009300): Train loss 10.812, Val loss 10.750\n",
            "Ep 1 (Iter 009600): Train loss 10.625, Val loss 10.625\n",
            "Ep 1 (Iter 009900): Train loss 10.000, Val loss 10.062\n",
            "Ep 1 (Iter 010200): Train loss 9.688, Val loss 9.875\n",
            "Ep 1 (Iter 010500): Train loss 9.438, Val loss 9.875\n",
            "Ep 1 (Iter 010800): Train loss 9.750, Val loss 9.812\n",
            "Ep 1 (Iter 011100): Train loss 9.750, Val loss 9.750\n",
            "Ep 1 (Iter 011400): Train loss 9.438, Val loss 9.750\n",
            "Ep 1 (Iter 011700): Train loss 9.500, Val loss 9.750\n",
            "Ep 1 (Iter 012000): Train loss 9.438, Val loss 9.688\n",
            "Ep 1 (Iter 012300): Train loss 9.250, Val loss 9.688\n",
            "Ep 1 (Iter 012600): Train loss 9.375, Val loss 9.688\n",
            "Ep 1 (Iter 012900): Train loss 9.500, Val loss 9.688\n",
            "Ep 1 (Iter 013200): Train loss 9.438, Val loss 9.688\n",
            "Ep 1 (Iter 013500): Train loss 9.500, Val loss 9.688\n",
            "Ep 1 (Iter 013800): Train loss 9.625, Val loss 9.688\n",
            "Saved model_checkpoints/model_pg_epoch_0.pth\n",
            "Ep 2 (Iter 014100): Train loss 9.688, Val loss 9.625\n",
            "Ep 2 (Iter 014400): Train loss 9.438, Val loss 9.625\n",
            "Ep 2 (Iter 014700): Train loss 9.188, Val loss 9.625\n",
            "Ep 2 (Iter 015000): Train loss 9.250, Val loss 9.625\n",
            "Ep 2 (Iter 015300): Train loss 9.438, Val loss 9.625\n",
            "Ep 2 (Iter 015600): Train loss 9.750, Val loss 9.625\n",
            "Ep 2 (Iter 015900): Train loss 9.625, Val loss 9.625\n",
            "Ep 2 (Iter 016200): Train loss 9.688, Val loss 9.625\n",
            "Ep 2 (Iter 016500): Train loss 9.812, Val loss 9.625\n",
            "Ep 2 (Iter 016800): Train loss 9.250, Val loss 9.625\n",
            "Ep 2 (Iter 017100): Train loss 9.438, Val loss 9.625\n",
            "Ep 2 (Iter 017400): Train loss 9.562, Val loss 9.625\n",
            "Ep 2 (Iter 017700): Train loss 9.750, Val loss 9.625\n",
            "Ep 2 (Iter 018000): Train loss 9.625, Val loss 9.625\n",
            "Ep 2 (Iter 018300): Train loss 9.500, Val loss 9.625\n",
            "Ep 2 (Iter 018600): Train loss 9.375, Val loss 9.625\n",
            "Ep 2 (Iter 018900): Train loss 9.938, Val loss 9.625\n",
            "Ep 2 (Iter 019200): Train loss 9.562, Val loss 9.625\n",
            "Ep 2 (Iter 019500): Train loss 9.875, Val loss 9.625\n",
            "Ep 2 (Iter 019800): Train loss 9.750, Val loss 9.625\n",
            "Ep 2 (Iter 020100): Train loss 10.125, Val loss 9.625\n",
            "Ep 2 (Iter 020400): Train loss 9.438, Val loss 9.625\n",
            "Ep 2 (Iter 020700): Train loss 10.000, Val loss 9.625\n",
            "Ep 2 (Iter 021000): Train loss 9.438, Val loss 9.625\n",
            "Ep 2 (Iter 021300): Train loss 9.750, Val loss 9.625\n",
            "Ep 2 (Iter 021600): Train loss 9.375, Val loss 9.625\n",
            "Ep 2 (Iter 021900): Train loss 9.562, Val loss 9.625\n",
            "Ep 2 (Iter 022200): Train loss 9.875, Val loss 9.625\n",
            "Ep 2 (Iter 022500): Train loss 9.312, Val loss 9.625\n",
            "Ep 2 (Iter 022800): Train loss 9.375, Val loss 9.625\n",
            "Ep 2 (Iter 023100): Train loss 9.438, Val loss 9.625\n",
            "Ep 2 (Iter 023400): Train loss 9.500, Val loss 9.625\n",
            "Ep 2 (Iter 023700): Train loss 9.438, Val loss 9.625\n",
            "Ep 2 (Iter 024000): Train loss 9.625, Val loss 9.625\n",
            "Ep 2 (Iter 024300): Train loss 9.562, Val loss 9.625\n",
            "Ep 2 (Iter 024600): Train loss 9.500, Val loss 9.625\n",
            "Ep 2 (Iter 024900): Train loss 10.375, Val loss 9.625\n",
            "Ep 2 (Iter 025200): Train loss 9.625, Val loss 9.625\n",
            "Ep 2 (Iter 025500): Train loss 9.375, Val loss 9.625\n",
            "Ep 2 (Iter 025800): Train loss 9.562, Val loss 9.625\n",
            "Ep 2 (Iter 026100): Train loss 9.562, Val loss 9.625\n",
            "Ep 2 (Iter 026400): Train loss 9.938, Val loss 9.625\n",
            "Ep 2 (Iter 026700): Train loss 9.562, Val loss 9.625\n",
            "Ep 2 (Iter 027000): Train loss 9.375, Val loss 9.625\n",
            "Ep 2 (Iter 027300): Train loss 9.375, Val loss 9.625\n",
            "Ep 2 (Iter 027600): Train loss 9.562, Val loss 9.625\n",
            "Saved model_checkpoints/model_pg_epoch_13843.pth\n",
            "Ep 3 (Iter 027900): Train loss 9.750, Val loss 9.625\n",
            "Ep 3 (Iter 028200): Train loss 9.688, Val loss 9.625\n",
            "Ep 3 (Iter 028500): Train loss 10.000, Val loss 9.625\n",
            "Ep 3 (Iter 028800): Train loss 9.250, Val loss 9.625\n",
            "Ep 3 (Iter 029100): Train loss 9.750, Val loss 9.625\n",
            "Ep 3 (Iter 029400): Train loss 9.750, Val loss 9.562\n",
            "Ep 3 (Iter 029700): Train loss 9.688, Val loss 9.625\n",
            "Ep 3 (Iter 030000): Train loss 9.250, Val loss 9.625\n",
            "Ep 3 (Iter 030300): Train loss 9.562, Val loss 9.625\n",
            "Ep 3 (Iter 030600): Train loss 9.562, Val loss 9.625\n",
            "Ep 3 (Iter 030900): Train loss 9.312, Val loss 9.625\n",
            "Ep 3 (Iter 031200): Train loss 9.562, Val loss 9.625\n",
            "Ep 3 (Iter 031500): Train loss 9.625, Val loss 9.625\n",
            "Ep 3 (Iter 031800): Train loss 9.875, Val loss 9.625\n",
            "Ep 3 (Iter 032100): Train loss 9.500, Val loss 9.625\n",
            "Ep 3 (Iter 032400): Train loss 9.688, Val loss 9.625\n",
            "Ep 3 (Iter 032700): Train loss 9.938, Val loss 9.625\n",
            "Ep 3 (Iter 033000): Train loss 9.688, Val loss 9.625\n",
            "Ep 3 (Iter 033300): Train loss 9.500, Val loss 9.625\n",
            "Ep 3 (Iter 033600): Train loss 9.500, Val loss 9.625\n",
            "Ep 3 (Iter 033900): Train loss 9.312, Val loss 9.625\n",
            "Ep 3 (Iter 034200): Train loss 9.125, Val loss 9.625\n",
            "Ep 3 (Iter 034500): Train loss 9.750, Val loss 9.625\n",
            "Ep 3 (Iter 034800): Train loss 9.438, Val loss 9.625\n",
            "Ep 3 (Iter 035100): Train loss 9.500, Val loss 9.625\n",
            "Ep 3 (Iter 035400): Train loss 9.375, Val loss 9.625\n",
            "Ep 3 (Iter 035700): Train loss 9.438, Val loss 9.625\n",
            "Ep 3 (Iter 036000): Train loss 9.750, Val loss 9.625\n",
            "Ep 3 (Iter 036300): Train loss 9.562, Val loss 9.625\n",
            "Ep 3 (Iter 036600): Train loss 9.625, Val loss 9.625\n",
            "Ep 3 (Iter 036900): Train loss 9.500, Val loss 9.625\n",
            "Ep 3 (Iter 037200): Train loss 9.812, Val loss 9.625\n",
            "Ep 3 (Iter 037500): Train loss 9.500, Val loss 9.625\n",
            "Ep 3 (Iter 037800): Train loss 9.375, Val loss 9.625\n",
            "Ep 3 (Iter 038100): Train loss 9.625, Val loss 9.625\n",
            "Ep 3 (Iter 038400): Train loss 9.375, Val loss 9.625\n",
            "Ep 3 (Iter 038700): Train loss 9.500, Val loss 9.625\n",
            "Ep 3 (Iter 039000): Train loss 9.312, Val loss 9.625\n",
            "Ep 3 (Iter 039300): Train loss 9.000, Val loss 9.625\n",
            "Ep 3 (Iter 039600): Train loss 9.438, Val loss 9.625\n",
            "Ep 3 (Iter 039900): Train loss 9.250, Val loss 9.625\n",
            "Ep 3 (Iter 040200): Train loss 9.688, Val loss 9.625\n",
            "Ep 3 (Iter 040500): Train loss 9.438, Val loss 9.625\n",
            "Ep 3 (Iter 040800): Train loss 9.312, Val loss 9.625\n",
            "Ep 3 (Iter 041100): Train loss 9.688, Val loss 9.625\n",
            "Ep 3 (Iter 041400): Train loss 9.188, Val loss 9.625\n",
            "Saved model_checkpoints/model_pg_epoch_13843.pth\n",
            "Maximum GPU memory allocated: 0.03 GB\n",
            "Training completed in 6.90 minutes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtjBWIN8ufS4",
        "outputId": "8c8964d9-d901-4a98-893d-d9128ab59244"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "usage: 3_pretrain.py [-h] [--output_dir OUTPUT_DIR] [--n_epochs N_EPOCHS]\n",
            "                     [--print_sample_iter PRINT_SAMPLE_ITER] [--eval_freq EVAL_FREQ]\n",
            "                     [--save_ckpt_freq SAVE_CKPT_FREQ] [--lr LR] [--batch_size BATCH_SIZE]\n",
            "                     [--debug DEBUG] [--max_text_len MAX_TEXT_LEN]\n",
            "                     [--resume_from_previous_training RESUME_FROM_PREVIOUS_TRAINING]\n",
            "                     [--push_to_hub_every_n_hours PUSH_TO_HUB_EVERY_N_HOURS]\n",
            "                     [--save_ckpt_freq_steps SAVE_CKPT_FREQ_STEPS]\n",
            "                     [--context_length CONTEXT_LENGTH]\n",
            "\n",
            "LLAMA3.2 Model Training Configuration\n",
            "\n",
            "options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --output_dir OUTPUT_DIR\n",
            "                        Directory where the model checkpoints will be saved\n",
            "  --n_epochs N_EPOCHS   Number of epochs to train the model\n",
            "  --print_sample_iter PRINT_SAMPLE_ITER\n",
            "                        Iterations between printing sample outputs\n",
            "  --eval_freq EVAL_FREQ\n",
            "                        Frequency of evaluations during training\n",
            "  --save_ckpt_freq SAVE_CKPT_FREQ\n",
            "                        Frequency of saving model checkpoints during training\n",
            "  --lr LR               Learning rate for the optimizer\n",
            "  --batch_size BATCH_SIZE\n",
            "                        Batch size for training\n",
            "  --debug DEBUG         Uses a very small model for debugging purposes\n",
            "  --max_text_len MAX_TEXT_LEN\n",
            "                        testing different text sizes.\n",
            "  --resume_from_previous_training RESUME_FROM_PREVIOUS_TRAINING\n",
            "                        whether or not to resume from saved previous training checkpoint\n",
            "  --push_to_hub_every_n_hours PUSH_TO_HUB_EVERY_N_HOURS\n",
            "                        how often to push to hub in hours.\n",
            "  --save_ckpt_freq_steps SAVE_CKPT_FREQ_STEPS\n",
            "                        how often to save the model checkpoint in steps\n",
            "  --context_length CONTEXT_LENGTH\n",
            "                        context length (default: 1024)\n"
          ]
        }
      ],
      "source": [
        "!python3 3_pretrain.py -h"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}