{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Zu7SXf76pnaC",
    "outputId": "965368ce-3e9d-4014-8ce4-1e7078a80021"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/MyDrive/Research/llama3\n",
      "3_pretrain.ipynb\t\tfunctions.py\t   previous_chapters.py\n",
      "3_pretrain.py\t\t\thf_cache\t   __pycache__\n",
      "cleaned_bhagavad_gita_data.txt\tllama_debug_model  standalone_llama32_bhagvada_gita_trainer.ipynb\n",
      "debug_dataloaders.py\t\tmodel_checkpoints  tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "%cd /content/drive/MyDrive/Research/llama3\n",
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UFXD73X3pfpH",
    "outputId": "bf1495a9-be82-4d3f-d4f8-dd3d430d720a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting blobfile>=3.0.0 (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1))\n",
      "  Downloading blobfile-3.0.0-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: huggingface_hub>=0.24.7 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (0.27.0)\n",
      "Collecting ipywidgets>=8.1.2 (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3))\n",
      "  Downloading ipywidgets-8.1.5-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.4 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 4)) (0.4.5)\n",
      "Requirement already satisfied: sentencepiece>=0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 5)) (0.2.0)\n",
      "Collecting pycryptodomex>=3.8 (from blobfile>=3.0.0->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1))\n",
      "  Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: urllib3<3,>=1.25.3 in /usr/local/lib/python3.10/dist-packages (from blobfile>=3.0.0->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1)) (2.2.3)\n",
      "Requirement already satisfied: lxml>=4.9 in /usr/local/lib/python3.10/dist-packages (from blobfile>=3.0.0->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1)) (5.3.0)\n",
      "Requirement already satisfied: filelock>=3.0 in /usr/local/lib/python3.10/dist-packages (from blobfile>=3.0.0->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 1)) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (4.12.2)\n",
      "Collecting comm>=0.1.3 (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3))\n",
      "  Downloading comm-0.2.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (7.34.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (5.7.1)\n",
      "Collecting widgetsnbextension~=4.0.12 (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3))\n",
      "  Downloading widgetsnbextension-4.0.13-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /usr/local/lib/python3.10/dist-packages (from ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (3.0.13)\n",
      "Requirement already satisfied: setuptools>=18.5 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (75.1.0)\n",
      "Collecting jedi>=0.16 (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3))\n",
      "  Downloading jedi-0.19.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (4.4.2)\n",
      "Requirement already satisfied: pickleshare in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.7.5)\n",
      "Requirement already satisfied: prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (3.0.48)\n",
      "Requirement already satisfied: pygments in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (2.18.0)\n",
      "Requirement already satisfied: backcall in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.2.0)\n",
      "Requirement already satisfied: matplotlib-inline in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.1.7)\n",
      "Requirement already satisfied: pexpect>4.3 in /usr/local/lib/python3.10/dist-packages (from ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface_hub>=0.24.7->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 2)) (2024.12.14)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.4 in /usr/local/lib/python3.10/dist-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /usr/local/lib/python3.10/dist-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /usr/local/lib/python3.10/dist-packages (from prompt-toolkit!=3.0.0,!=3.0.1,<3.1.0,>=2.0.0->ipython>=6.1.0->ipywidgets>=8.1.2->-r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt (line 3)) (0.2.13)\n",
      "Downloading blobfile-3.0.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.4/75.4 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading ipywidgets-8.1.5-py3-none-any.whl (139 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.8/139.8 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading comm-0.2.2-py3-none-any.whl (7.2 kB)\n",
      "Downloading pycryptodomex-3.21.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m47.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading widgetsnbextension-4.0.13-py3-none-any.whl (2.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading jedi-0.19.2-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: widgetsnbextension, pycryptodomex, jedi, comm, blobfile, ipywidgets\n",
      "  Attempting uninstall: widgetsnbextension\n",
      "    Found existing installation: widgetsnbextension 3.6.10\n",
      "    Uninstalling widgetsnbextension-3.6.10:\n",
      "      Successfully uninstalled widgetsnbextension-3.6.10\n",
      "  Attempting uninstall: ipywidgets\n",
      "    Found existing installation: ipywidgets 7.7.1\n",
      "    Uninstalling ipywidgets-7.7.1:\n",
      "      Successfully uninstalled ipywidgets-7.7.1\n",
      "Successfully installed blobfile-3.0.0 comm-0.2.2 ipywidgets-8.1.5 jedi-0.19.2 pycryptodomex-3.21.0 widgetsnbextension-4.0.13\n"
     ]
    }
   ],
   "source": [
    "!pip install -r https://raw.githubusercontent.com/rasbt/LLMs-from-scratch/refs/heads/main/ch05/07_gpt_to_llama/requirements-extra.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "23JMDjMtqGyr",
    "outputId": "7d45347a-6be6-4dbc-f69b-1be93698c599"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/480.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m480.6/480.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/116.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m179.3/179.3 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install blobfile datasets --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UqxQ62Iqp-xk",
    "outputId": "b771858d-358a-46b0-b5a0-159c1f724b56"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "blobfile version: 3.0.0\n",
      "huggingface_hub version: 0.27.0\n",
      "torch version: 2.5.1+cu121\n"
     ]
    }
   ],
   "source": [
    "from importlib.metadata import version\n",
    "\n",
    "pkgs = [\n",
    "    \"blobfile\",         # to download pretrained weights\n",
    "    \"huggingface_hub\",  # to download pretrained weights\n",
    "    # \"tiktoken\",         # to implement the tokenizer\n",
    "    \"torch\",            # to implement the model\n",
    "]\n",
    "for p in pkgs:\n",
    "    print(f\"{p} version: {version(p)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aPMYqSeKuIpM",
    "outputId": "e7f94d8b-ae9c-486c-ca09-c4bea39efee4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting 3_pretrain.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile 3_pretrain.py\n",
    "# # code from https://github.com/rasbt/LLMs-from-scratch/blob/main/appendix-D/01_main-chapter-code/appendix-D.ipynb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LJbmfOqO4K8q",
    "outputId": "6bec0ec4-1c3f-4b7f-d5d3-5e681c373366"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting previous_chapters.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile previous_chapters.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lZB4TjOvCbIr",
    "outputId": "ff6ca4cc-ac66-416a-9a94-7fd8a929a318"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting debug_dataloaders.py\n"
     ]
    }
   ],
   "source": [
    "# %%writefile debug_dataloaders.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WzdzMhL7GmrS",
    "outputId": "803452e3-753e-48d9-87f0-7b1383ea2bc7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3_pretrain.ipynb\t\tfunctions.py\t   previous_chapters.py\n",
      "3_pretrain.py\t\t\thf_cache\t   __pycache__\n",
      "cleaned_bhagavad_gita_data.txt\tllama_debug_model  standalone_llama32_bhagvada_gita_trainer.ipynb\n",
      "debug_dataloaders.py\t\tmodel_checkpoints  tokenizer.json\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWweXfduhQr9",
    "outputId": "44e29d5a-69a9-4dd9-d9f6-78ea54a93135"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------\n",
      "DEBUG MODE\n",
      "---------------------\n",
      "New RoPE theta (i.e. LLAMA32_CONFIG[\"rope_base\"]): 4367.320268554277\n",
      "The following is expected to print True to confirm buffers are reused instead of being (wastefully) recreated:\n",
      "True\n",
      "True\n",
      "True\n",
      "Total number of parameters: 801,288\n",
      "\n",
      "Total number of unique parameters: 401,240\n",
      "float32 (PyTorch default): 0.01 GB\n",
      "bfloat16: 0.00 GB\n",
      "device: cuda\n",
      "\n",
      "\n",
      "args.resume_from_previous_training: False\n",
      "\n",
      "\n",
      "starting new model from scratch\n",
      "len. train_loader: 13844\n",
      "len.val_loader: 1581\n",
      " warmup_steps: 8306\n",
      "Training ...\n",
      "Ep 1 (Iter 000000): Train loss 10.750, Val loss 10.938\n",
      "Ep 1 (Iter 000300): Train loss 10.875, Val loss 10.875\n",
      "Ep 1 (Iter 000600): Train loss 10.875, Val loss 10.875\n",
      "Ep 1 (Iter 000900): Train loss 10.938, Val loss 10.812\n",
      "Ep 1 (Iter 001200): Train loss 10.938, Val loss 10.812\n",
      "Ep 1 (Iter 001500): Train loss 10.875, Val loss 10.812\n",
      "Ep 1 (Iter 001800): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 002100): Train loss 10.938, Val loss 10.812\n",
      "Ep 1 (Iter 002400): Train loss 10.875, Val loss 10.812\n",
      "Ep 1 (Iter 002700): Train loss 10.750, Val loss 10.812\n",
      "Ep 1 (Iter 003000): Train loss 10.875, Val loss 10.812\n",
      "Ep 1 (Iter 003300): Train loss 10.875, Val loss 10.812\n",
      "Ep 1 (Iter 003600): Train loss 10.875, Val loss 10.812\n",
      "Ep 1 (Iter 003900): Train loss 10.688, Val loss 10.812\n",
      "Ep 1 (Iter 004200): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 004500): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 004800): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 005100): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 005400): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 005700): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 006000): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 006300): Train loss 10.750, Val loss 10.812\n",
      "Ep 1 (Iter 006600): Train loss 10.750, Val loss 10.812\n",
      "Ep 1 (Iter 006900): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 007200): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 007500): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 007800): Train loss 10.750, Val loss 10.812\n",
      "Ep 1 (Iter 008100): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 008400): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 008700): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 009000): Train loss 10.812, Val loss 10.812\n",
      "Ep 1 (Iter 009300): Train loss 10.812, Val loss 10.750\n",
      "Ep 1 (Iter 009600): Train loss 10.625, Val loss 10.625\n",
      "Ep 1 (Iter 009900): Train loss 10.000, Val loss 10.062\n",
      "Ep 1 (Iter 010200): Train loss 9.688, Val loss 9.875\n",
      "Ep 1 (Iter 010500): Train loss 9.438, Val loss 9.875\n",
      "Ep 1 (Iter 010800): Train loss 9.750, Val loss 9.812\n",
      "Ep 1 (Iter 011100): Train loss 9.750, Val loss 9.750\n",
      "Ep 1 (Iter 011400): Train loss 9.438, Val loss 9.750\n",
      "Ep 1 (Iter 011700): Train loss 9.500, Val loss 9.750\n",
      "Ep 1 (Iter 012000): Train loss 9.438, Val loss 9.688\n",
      "Ep 1 (Iter 012300): Train loss 9.250, Val loss 9.688\n",
      "Ep 1 (Iter 012600): Train loss 9.375, Val loss 9.688\n",
      "Ep 1 (Iter 012900): Train loss 9.500, Val loss 9.688\n",
      "Ep 1 (Iter 013200): Train loss 9.438, Val loss 9.688\n",
      "Ep 1 (Iter 013500): Train loss 9.500, Val loss 9.688\n",
      "Ep 1 (Iter 013800): Train loss 9.625, Val loss 9.688\n",
      "Saved model_checkpoints/model_pg_epoch_0.pth\n",
      "Ep 2 (Iter 014100): Train loss 9.688, Val loss 9.625\n",
      "Ep 2 (Iter 014400): Train loss 9.438, Val loss 9.625\n",
      "Ep 2 (Iter 014700): Train loss 9.188, Val loss 9.625\n",
      "Ep 2 (Iter 015000): Train loss 9.250, Val loss 9.625\n",
      "Ep 2 (Iter 015300): Train loss 9.438, Val loss 9.625\n",
      "Ep 2 (Iter 015600): Train loss 9.750, Val loss 9.625\n",
      "Ep 2 (Iter 015900): Train loss 9.625, Val loss 9.625\n",
      "Ep 2 (Iter 016200): Train loss 9.688, Val loss 9.625\n",
      "Ep 2 (Iter 016500): Train loss 9.812, Val loss 9.625\n",
      "Ep 2 (Iter 016800): Train loss 9.250, Val loss 9.625\n",
      "Ep 2 (Iter 017100): Train loss 9.438, Val loss 9.625\n",
      "Ep 2 (Iter 017400): Train loss 9.562, Val loss 9.625\n",
      "Ep 2 (Iter 017700): Train loss 9.750, Val loss 9.625\n",
      "Ep 2 (Iter 018000): Train loss 9.625, Val loss 9.625\n",
      "Ep 2 (Iter 018300): Train loss 9.500, Val loss 9.625\n",
      "Ep 2 (Iter 018600): Train loss 9.375, Val loss 9.625\n",
      "Ep 2 (Iter 018900): Train loss 9.938, Val loss 9.625\n",
      "Ep 2 (Iter 019200): Train loss 9.562, Val loss 9.625\n",
      "Ep 2 (Iter 019500): Train loss 9.875, Val loss 9.625\n",
      "Ep 2 (Iter 019800): Train loss 9.750, Val loss 9.625\n",
      "Ep 2 (Iter 020100): Train loss 10.125, Val loss 9.625\n",
      "Ep 2 (Iter 020400): Train loss 9.438, Val loss 9.625\n",
      "Ep 2 (Iter 020700): Train loss 10.000, Val loss 9.625\n",
      "Ep 2 (Iter 021000): Train loss 9.438, Val loss 9.625\n",
      "Ep 2 (Iter 021300): Train loss 9.750, Val loss 9.625\n",
      "Ep 2 (Iter 021600): Train loss 9.375, Val loss 9.625\n",
      "Ep 2 (Iter 021900): Train loss 9.562, Val loss 9.625\n",
      "Ep 2 (Iter 022200): Train loss 9.875, Val loss 9.625\n",
      "Ep 2 (Iter 022500): Train loss 9.312, Val loss 9.625\n",
      "Ep 2 (Iter 022800): Train loss 9.375, Val loss 9.625\n",
      "Ep 2 (Iter 023100): Train loss 9.438, Val loss 9.625\n",
      "Ep 2 (Iter 023400): Train loss 9.500, Val loss 9.625\n",
      "Ep 2 (Iter 023700): Train loss 9.438, Val loss 9.625\n",
      "Ep 2 (Iter 024000): Train loss 9.625, Val loss 9.625\n",
      "Ep 2 (Iter 024300): Train loss 9.562, Val loss 9.625\n",
      "Ep 2 (Iter 024600): Train loss 9.500, Val loss 9.625\n",
      "Ep 2 (Iter 024900): Train loss 10.375, Val loss 9.625\n",
      "Ep 2 (Iter 025200): Train loss 9.625, Val loss 9.625\n",
      "Ep 2 (Iter 025500): Train loss 9.375, Val loss 9.625\n",
      "Ep 2 (Iter 025800): Train loss 9.562, Val loss 9.625\n",
      "Ep 2 (Iter 026100): Train loss 9.562, Val loss 9.625\n",
      "Ep 2 (Iter 026400): Train loss 9.938, Val loss 9.625\n",
      "Ep 2 (Iter 026700): Train loss 9.562, Val loss 9.625\n",
      "Ep 2 (Iter 027000): Train loss 9.375, Val loss 9.625\n",
      "Ep 2 (Iter 027300): Train loss 9.375, Val loss 9.625\n",
      "Ep 2 (Iter 027600): Train loss 9.562, Val loss 9.625\n",
      "Saved model_checkpoints/model_pg_epoch_13843.pth\n",
      "Ep 3 (Iter 027900): Train loss 9.750, Val loss 9.625\n",
      "Ep 3 (Iter 028200): Train loss 9.688, Val loss 9.625\n",
      "Ep 3 (Iter 028500): Train loss 10.000, Val loss 9.625\n",
      "Ep 3 (Iter 028800): Train loss 9.250, Val loss 9.625\n",
      "Ep 3 (Iter 029100): Train loss 9.750, Val loss 9.625\n",
      "Ep 3 (Iter 029400): Train loss 9.750, Val loss 9.562\n",
      "Ep 3 (Iter 029700): Train loss 9.688, Val loss 9.625\n",
      "Ep 3 (Iter 030000): Train loss 9.250, Val loss 9.625\n",
      "Ep 3 (Iter 030300): Train loss 9.562, Val loss 9.625\n",
      "Ep 3 (Iter 030600): Train loss 9.562, Val loss 9.625\n",
      "Ep 3 (Iter 030900): Train loss 9.312, Val loss 9.625\n",
      "Ep 3 (Iter 031200): Train loss 9.562, Val loss 9.625\n",
      "Ep 3 (Iter 031500): Train loss 9.625, Val loss 9.625\n",
      "Ep 3 (Iter 031800): Train loss 9.875, Val loss 9.625\n",
      "Ep 3 (Iter 032100): Train loss 9.500, Val loss 9.625\n",
      "Ep 3 (Iter 032400): Train loss 9.688, Val loss 9.625\n",
      "Ep 3 (Iter 032700): Train loss 9.938, Val loss 9.625\n",
      "Ep 3 (Iter 033000): Train loss 9.688, Val loss 9.625\n",
      "Ep 3 (Iter 033300): Train loss 9.500, Val loss 9.625\n",
      "Ep 3 (Iter 033600): Train loss 9.500, Val loss 9.625\n",
      "Ep 3 (Iter 033900): Train loss 9.312, Val loss 9.625\n",
      "Ep 3 (Iter 034200): Train loss 9.125, Val loss 9.625\n",
      "Ep 3 (Iter 034500): Train loss 9.750, Val loss 9.625\n",
      "Ep 3 (Iter 034800): Train loss 9.438, Val loss 9.625\n",
      "Ep 3 (Iter 035100): Train loss 9.500, Val loss 9.625\n",
      "Ep 3 (Iter 035400): Train loss 9.375, Val loss 9.625\n",
      "Ep 3 (Iter 035700): Train loss 9.438, Val loss 9.625\n",
      "Ep 3 (Iter 036000): Train loss 9.750, Val loss 9.625\n",
      "Ep 3 (Iter 036300): Train loss 9.562, Val loss 9.625\n",
      "Ep 3 (Iter 036600): Train loss 9.625, Val loss 9.625\n",
      "Ep 3 (Iter 036900): Train loss 9.500, Val loss 9.625\n",
      "Ep 3 (Iter 037200): Train loss 9.812, Val loss 9.625\n",
      "Ep 3 (Iter 037500): Train loss 9.500, Val loss 9.625\n",
      "Ep 3 (Iter 037800): Train loss 9.375, Val loss 9.625\n",
      "Ep 3 (Iter 038100): Train loss 9.625, Val loss 9.625\n",
      "Ep 3 (Iter 038400): Train loss 9.375, Val loss 9.625\n",
      "Ep 3 (Iter 038700): Train loss 9.500, Val loss 9.625\n",
      "Ep 3 (Iter 039000): Train loss 9.312, Val loss 9.625\n",
      "Ep 3 (Iter 039300): Train loss 9.000, Val loss 9.625\n",
      "Ep 3 (Iter 039600): Train loss 9.438, Val loss 9.625\n",
      "Ep 3 (Iter 039900): Train loss 9.250, Val loss 9.625\n",
      "Ep 3 (Iter 040200): Train loss 9.688, Val loss 9.625\n",
      "Ep 3 (Iter 040500): Train loss 9.438, Val loss 9.625\n",
      "Ep 3 (Iter 040800): Train loss 9.312, Val loss 9.625\n",
      "Ep 3 (Iter 041100): Train loss 9.688, Val loss 9.625\n",
      "Ep 3 (Iter 041400): Train loss 9.188, Val loss 9.625\n",
      "Saved model_checkpoints/model_pg_epoch_13843.pth\n",
      "Maximum GPU memory allocated: 0.03 GB\n",
      "Training completed in 6.90 minutes.\n"
     ]
    }
   ],
   "source": [
    "!python 3_pretrain.py \\\n",
    " --resume_from_previous_training False\\\n",
    " --n_epochs 3 \\\n",
    " --print_sample_iter 2_000 \\\n",
    " --save_ckpt_freq 2_000 \\\n",
    " --batch_size 2 \\\n",
    " --eval_freq 300 \\\n",
    " --output_dir model_checkpoints \\\n",
    " --save_ckpt_freq_steps 100000 \\\n",
    " --debug True \\\n",
    " --lr 5e-4 \\\n",
    "\n",
    "#  n_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PtjBWIN8ufS4",
    "outputId": "8c8964d9-d901-4a98-893d-d9128ab59244"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: 3_pretrain.py [-h] [--output_dir OUTPUT_DIR] [--n_epochs N_EPOCHS]\n",
      "                     [--print_sample_iter PRINT_SAMPLE_ITER] [--eval_freq EVAL_FREQ]\n",
      "                     [--save_ckpt_freq SAVE_CKPT_FREQ] [--lr LR] [--batch_size BATCH_SIZE]\n",
      "                     [--debug DEBUG] [--max_text_len MAX_TEXT_LEN]\n",
      "                     [--resume_from_previous_training RESUME_FROM_PREVIOUS_TRAINING]\n",
      "                     [--push_to_hub_every_n_hours PUSH_TO_HUB_EVERY_N_HOURS]\n",
      "                     [--save_ckpt_freq_steps SAVE_CKPT_FREQ_STEPS]\n",
      "                     [--context_length CONTEXT_LENGTH]\n",
      "\n",
      "LLAMA3.2 Model Training Configuration\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --output_dir OUTPUT_DIR\n",
      "                        Directory where the model checkpoints will be saved\n",
      "  --n_epochs N_EPOCHS   Number of epochs to train the model\n",
      "  --print_sample_iter PRINT_SAMPLE_ITER\n",
      "                        Iterations between printing sample outputs\n",
      "  --eval_freq EVAL_FREQ\n",
      "                        Frequency of evaluations during training\n",
      "  --save_ckpt_freq SAVE_CKPT_FREQ\n",
      "                        Frequency of saving model checkpoints during training\n",
      "  --lr LR               Learning rate for the optimizer\n",
      "  --batch_size BATCH_SIZE\n",
      "                        Batch size for training\n",
      "  --debug DEBUG         Uses a very small model for debugging purposes\n",
      "  --max_text_len MAX_TEXT_LEN\n",
      "                        testing different text sizes.\n",
      "  --resume_from_previous_training RESUME_FROM_PREVIOUS_TRAINING\n",
      "                        whether or not to resume from saved previous training checkpoint\n",
      "  --push_to_hub_every_n_hours PUSH_TO_HUB_EVERY_N_HOURS\n",
      "                        how often to push to hub in hours.\n",
      "  --save_ckpt_freq_steps SAVE_CKPT_FREQ_STEPS\n",
      "                        how often to save the model checkpoint in steps\n",
      "  --context_length CONTEXT_LENGTH\n",
      "                        context length (default: 1024)\n"
     ]
    }
   ],
   "source": [
    "!python3 3_pretrain.py -h"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
