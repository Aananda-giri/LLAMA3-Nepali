{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347485696\n"
     ]
    }
   ],
   "source": [
    "# calculate model size\n",
    "\n",
    "# Dummy Model configurations for Llama 3.2 1B\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,\n",
    "    \"emb_dim\": 2048,\n",
    "    \"n_heads\": 32,\n",
    "    \"n_layers\": 16,\n",
    "    \"hidden_dim\": 8192,\n",
    "    \"n_kv_groups\": 8,\n",
    "}\n",
    "\n",
    "\n",
    "def calculate_size(LLAMA32_CONFIG, return_it=False):\n",
    "    # Extract parameters\n",
    "    vocab_size = LLAMA32_CONFIG[\"vocab_size\"]\n",
    "    emb_dim = LLAMA32_CONFIG[\"emb_dim\"]\n",
    "    n_heads = LLAMA32_CONFIG[\"n_heads\"]\n",
    "    n_layers = LLAMA32_CONFIG[\"n_layers\"]\n",
    "    hidden_dim = LLAMA32_CONFIG[\"hidden_dim\"]\n",
    "    n_kv_groups = LLAMA32_CONFIG[\"n_kv_groups\"]\n",
    "\n",
    "    # Embedding Layer\n",
    "    embedding_size = vocab_size * emb_dim\n",
    "\n",
    "    # Attention Mechanism (per layer)\n",
    "    # QKV projections\n",
    "    qkv_size = 3 * (emb_dim * emb_dim)\n",
    "    # Output projection\n",
    "    output_proj_size = emb_dim * emb_dim\n",
    "    # Adjustments for key-value groups\n",
    "    qkv_size_grouped = 2 * (emb_dim * emb_dim // n_kv_groups)  # key and value projections\n",
    "    attention_size = qkv_size + qkv_size_grouped + output_proj_size\n",
    "\n",
    "    # Feedforward Layer (per layer)\n",
    "    feedforward_size = (emb_dim * hidden_dim) + (hidden_dim * emb_dim)\n",
    "\n",
    "    # Layer Normalization (per layer)\n",
    "    layer_norm_size = 2 * emb_dim\n",
    "\n",
    "    # Total per-layer size\n",
    "    layer_size = attention_size + feedforward_size + layer_norm_size\n",
    "\n",
    "    # Stacked layers\n",
    "    total_layer_size = layer_size * n_layers\n",
    "\n",
    "    # Output Projection\n",
    "    output_projection_size = emb_dim * vocab_size\n",
    "\n",
    "    # Total Model Size\n",
    "    total_model_size = embedding_size + total_layer_size + output_projection_size\n",
    "    \n",
    "    if return_it:return total_model_size\n",
    "    print(total_model_size)\n",
    "\n",
    "calculate_size(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3320487936\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Llama 3.2 3B\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # Context length\n",
    "    \"emb_dim\": 3072,            # Embedding dimension\n",
    "    \"n_heads\": 24,              # Number of attention heads\n",
    "    \"n_layers\": 28,             # Number of layers\n",
    "    \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "\n",
    "calculate_size(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1347485696\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Llama 3.2 1B\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 131_072,  # Context length\n",
    "    \"emb_dim\": 2048,            # Embedding dimension\n",
    "    \"n_heads\": 32,              # Number of attention heads\n",
    "    \"n_layers\": 16,             # Number of layers\n",
    "    \"hidden_dim\": 8192,         # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "calculate_size(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom smaller models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " model size: 205176832\n",
      "these values should be True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "def check_config(LLAMA32_CONFIG):\n",
    "    '''\n",
    "    emb_dim//n_heads must be even\n",
    "    emb_dim must be divisible by num_kv_groups\n",
    "    '''\n",
    "    print(f'these values should be True')\n",
    "    print(LLAMA32_CONFIG[\"emb_dim\"] % LLAMA32_CONFIG[\"n_heads\"] == 0)\n",
    "    print((LLAMA32_CONFIG[\"emb_dim\"] // LLAMA32_CONFIG[\"n_kv_groups\"])%2 == 0)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Llama 3.2 200M\n",
    "LLAMA32_CONFIG = {\n",
    "    \"vocab_size\": 50_000,       # 128_256 reduced vocabulary size\n",
    "    \"context_length\": 2048,     # 131_072 reduced Context length (unrelated to model size)\n",
    "    \"emb_dim\": 1024,            # 2048 reduced Embedding dimension\n",
    "    \"n_heads\": 16,              # 32 reduced Number of attention heads\n",
    "    \"n_layers\": 8,             # 16 reduced Number of layers\n",
    "    \"hidden_dim\": 4096,         # 8192 Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 8,           # 8 Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # 500_000 The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f' model size: {calculate_size(LLAMA32_CONFIG, return_it=True)}')\n",
    "check_config(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Debug mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2053280\n"
     ]
    }
   ],
   "source": [
    "# use this one (not above)?\n",
    "import torch\n",
    "\n",
    "'''\n",
    "emb_dim//num_heads must be even\n",
    "emb_dim must be divisible by num_kv_groups\n",
    "'''\n",
    "\n",
    "# Debug mode\n",
    "LLAMA32_CONFIG = {\n",
    "    # d_out = emb_dim\n",
    "    # Embedding dimension <d_out // num_heads> must be even\n",
    "    \"vocab_size\": 128_256,      # Vocabulary size\n",
    "    \"context_length\": 10,  # Context length\n",
    "    # d_in=d_out=emb_dim,\n",
    "    # d_out must be divisible by num_heads\n",
    "    \"emb_dim\": 8,            # Embedding dimension\n",
    "    # (num_heads must be divisible by num_kv_groups)\n",
    "    \"n_heads\": 4,              # Number of attention heads\n",
    "    \"n_layers\": 2,             # Number of layers\n",
    "    \"hidden_dim\": 16,         # Size of the intermediate dimension in FeedForward\n",
    "    \"n_kv_groups\": 2,           # Key-Value groups for grouped-query attention\n",
    "    \"rope_base\": 500_000.0,     # The base in RoPE's \"theta\"\n",
    "    \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "    \"rope_freq\": {              # RoPE frequency scaling\n",
    "        \"factor\": 32.0,\n",
    "        \"low_freq_factor\": 1.0,\n",
    "        \"high_freq_factor\": 4.0,\n",
    "        \"original_context_length\": 8192,\n",
    "    }\n",
    "}\n",
    "calculate_size(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "205176832\n"
     ]
    }
   ],
   "source": [
    "# Llama 3.2 1B\n",
    "\n",
    "calculate_size(LLAMA32_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.debug:\n",
    "    LLAMA_CONFIG = {\n",
    "        \"vocab_size\": 50_000,        # Small vocab size for quick embedding testing\n",
    "        \"context_length\": 8,      # Very short context length\n",
    "        \"emb_dim\": 16,            # Minimal embedding dimension\n",
    "        \"n_heads\": 2,             # Minimal number of attention heads\n",
    "        \"n_layers\": 2,            # Minimal number of transformer layers\n",
    "        \"hidden_dim\": 64,         # Scaled-down feedforward dimension\n",
    "        \"n_kv_groups\": 1,         # Simplified attention grouping\n",
    "        \"drop_rate\": 0.0,         # Dropout deactivated for deterministic debugging\n",
    "        \"qkv_bias\": False         # Simplified attention mechanism\n",
    "    }\n",
    "\n",
    "else:\n",
    "    # Llama 3.2 200M\n",
    "    LLAMA32_CONFIG = {\n",
    "        \"vocab_size\": 50_000,       # 128_256 reduced vocabulary size\n",
    "        \"context_length\": 2048,     # 131_072 reduced Context length (unrelated to model size)\n",
    "        \"emb_dim\": 1024,            # 2048 reduced Embedding dimension\n",
    "        \"n_heads\": 16,              # 32 reduced Number of attention heads\n",
    "        \"n_layers\": 8,             # 16 reduced Number of layers\n",
    "        \"hidden_dim\": 4096,         # 8192 Size of the intermediate dimension in FeedForward\n",
    "        \"n_kv_groups\": 8,           # 8 Key-Value groups for grouped-query attention\n",
    "        \"rope_base\": 500_000.0,     # 500_000 The base in RoPE's \"theta\"\n",
    "        \"dtype\": torch.bfloat16,    # Lower-precision dtype to reduce memory usage\n",
    "        \"rope_freq\": {              # RoPE frequency scaling\n",
    "            \"factor\": 32.0,\n",
    "            \"low_freq_factor\": 1.0,\n",
    "            \"high_freq_factor\": 4.0,\n",
    "            \"original_context_length\": 8192,\n",
    "        }\n",
    "    }\n",
    "\n",
    "calculate_size(LLAMA124M_CONFIG)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "machine_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
