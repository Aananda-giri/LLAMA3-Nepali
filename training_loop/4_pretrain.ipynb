{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Sebastian Raschka under Apache License 2.0 (see LICENSE.txt).\n",
    "# Source for \"Build a Large Language Model From Scratch\"\n",
    "#   - https://www.manning.com/books/build-a-large-language-model-from-scratch\n",
    "# Code: https://github.com/rasbt/LLMs-from-scratch\n",
    "\n",
    "\"\"\"\n",
    "Script for pretraining a small GPT-2 124M parameter model\n",
    "on books from Project Gutenberg.\n",
    "\n",
    "Before running this script, make sure you downloaded and\n",
    "processed the dataset as described in the README.md.\n",
    "\"\"\"\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from pathlib import Path\n",
    "import time\n",
    "import tiktoken\n",
    "import torch\n",
    "from previous_chapters import (\n",
    "    create_dataloader_v1,\n",
    "    GPTModel,\n",
    "    generate_and_print_sample,\n",
    "    calc_loss_batch,\n",
    "    evaluate_model,\n",
    "    plot_losses\n",
    ")\n",
    "\n",
    "\n",
    "def read_text_file(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        text_data = file.read()\n",
    "    return text_data\n",
    "\n",
    "\n",
    "def create_dataloaders(text_data, train_ratio, batch_size, max_length, stride, num_workers=0):\n",
    "    split_idx = int(train_ratio * len(text_data))\n",
    "    train_loader = create_dataloader_v1(\n",
    "        text_data[:split_idx],\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        drop_last=True,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    val_loader = create_dataloader_v1(\n",
    "        text_data[split_idx:],\n",
    "        batch_size=batch_size,\n",
    "        max_length=max_length,\n",
    "        stride=stride,\n",
    "        drop_last=False,\n",
    "        shuffle=False,\n",
    "        num_workers=num_workers\n",
    "    )\n",
    "    return train_loader, val_loader\n",
    "\n",
    "\n",
    "def convert_time(seconds):\n",
    "    hours, rem = divmod(seconds, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return int(hours), int(minutes), int(seconds)\n",
    "\n",
    "\n",
    "def print_eta(start_time, book_start_time, index, total_files):\n",
    "    book_end_time = time.time()  # End time of processing this book\n",
    "    elapsed_time = book_end_time - book_start_time\n",
    "    total_elapsed_time = book_end_time - start_time\n",
    "    books_remaining = total_files - index\n",
    "    average_time_per_book = total_elapsed_time / index\n",
    "    eta = average_time_per_book * books_remaining\n",
    "\n",
    "    book_h, book_m, book_s = convert_time(elapsed_time)\n",
    "    total_h, total_m, total_s = convert_time(total_elapsed_time)\n",
    "    eta_h, eta_m, eta_s = convert_time(eta)\n",
    "\n",
    "    print(f\"Book processed {book_h}h {book_m}m {book_s}s\"\n",
    "          f\"\\nTotal time elapsed {total_h}h {total_m}m {total_s}s\"\n",
    "          f\"\\nETA for remaining books: {eta_h}h {eta_m}m {eta_s}s\")\n",
    "\n",
    "\n",
    "def train_model_simple(model, optimizer, device, n_epochs,\n",
    "                       eval_freq, eval_iter, print_sample_iter, start_context,\n",
    "                       output_dir, save_ckpt_freq, tokenizer,\n",
    "                       batch_size=1024, train_ratio=0.90):\n",
    "\n",
    "    train_losses, val_losses, track_tokens_seen = [], [], []\n",
    "    tokens_seen = 0\n",
    "    global_step = -1\n",
    "    start_time = time.time()\n",
    "\n",
    "    try:\n",
    "        for epoch in range(n_epochs):\n",
    "\n",
    "            # Iterate over the books in the training corpus\n",
    "            for index, file_path in enumerate(all_files, 1):\n",
    "                book_start_time = time.time()\n",
    "                text_data = read_text_file(file_path) + \" <|endoftext|> \"\n",
    "                print(f\"Tokenizing file {index} of {total_files}: {file_path}\")\n",
    "\n",
    "                # Initialize new data loaders for each book\n",
    "                train_loader, val_loader = create_dataloaders(\n",
    "                    text_data,\n",
    "                    train_ratio=train_ratio,\n",
    "                    batch_size=batch_size,\n",
    "                    max_length=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    stride=GPT_CONFIG_124M[\"context_length\"],\n",
    "                    num_workers=0\n",
    "                )\n",
    "                print(\"Training ...\")\n",
    "                model.train()\n",
    "                for input_batch, target_batch in train_loader:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss = calc_loss_batch(input_batch, target_batch, model, device)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    tokens_seen += input_batch.numel()\n",
    "                    global_step += 1\n",
    "\n",
    "                    # Optional evaluation step\n",
    "                    if global_step % eval_freq == 0:\n",
    "                        train_loss, val_loss = evaluate_model(\n",
    "                            model, train_loader, val_loader, device, eval_iter)\n",
    "                        train_losses.append(train_loss)\n",
    "                        val_losses.append(val_loss)\n",
    "                        track_tokens_seen.append(tokens_seen)\n",
    "                        print(f\"Ep {epoch+1} (Step {global_step}): \"\n",
    "                              f\"Train loss {train_loss:.3f}, Val loss {val_loss:.3f}\")\n",
    "\n",
    "                    # Generate text passage\n",
    "                    if global_step % print_sample_iter == 0:\n",
    "                        generate_and_print_sample(\n",
    "                            model, tokenizer, device, start_context\n",
    "                        )\n",
    "\n",
    "                if global_step % save_ckpt_freq:\n",
    "                    file_name = output_dir / f\"model_pg_{global_step}.pth\"\n",
    "                    torch.save(model.state_dict(), file_name)\n",
    "                    print(f\"Saved {file_name}\")\n",
    "\n",
    "                print_eta(start_time, book_start_time, index, total_files)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        file_name = output_dir / f\"model_pg_{global_step}_interrupted.pth\"\n",
    "        torch.save(model.state_dict(), file_name)\n",
    "        print(f\"Saved {file_name}\")\n",
    "\n",
    "    return train_losses, val_losses, track_tokens_seen\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    parser = argparse.ArgumentParser(description='GPT Model Training Configuration')\n",
    "\n",
    "    parser.add_argument('--data_dir', type=str, default='gutenberg/data',\n",
    "                        help='Directory containing the training data')\n",
    "    parser.add_argument('--output_dir', type=str, default='model_checkpoints',\n",
    "                        help='Directory where the model checkpoints will be saved')\n",
    "    parser.add_argument('--n_epochs', type=int, default=1,\n",
    "                        help='Number of epochs to train the model')\n",
    "    parser.add_argument('--print_sample_iter', type=int, default=1000,\n",
    "                        help='Iterations between printing sample outputs')\n",
    "    parser.add_argument('--eval_freq', type=int, default=100,\n",
    "                        help='Frequency of evaluations during training')\n",
    "    parser.add_argument('--save_ckpt_freq', type=int, default=100_000,\n",
    "                        help='Frequency of saving model checkpoints during training')\n",
    "    parser.add_argument('--lr', type=float, default=5e-4,\n",
    "                        help='Learning rate for the optimizer')\n",
    "    parser.add_argument('--batch_size', type=int, default=4,\n",
    "                        help='Batch size for training')\n",
    "    parser.add_argument('--debug', type=bool, default=False,\n",
    "                        help='Uses a very small model for debugging purposes')\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    if args.debug:\n",
    "        GPT_CONFIG_124M = {\n",
    "            \"vocab_size\": 50257,     # Vocabulary size\n",
    "            \"context_length\": 10,    # Context length\n",
    "            \"emb_dim\": 12,           # Embedding dimension\n",
    "            \"n_heads\": 2,            # Number of attention heads\n",
    "            \"n_layers\": 2,           # Number of layers\n",
    "            \"drop_rate\": 0.0,        # Dropout rate, deactivated via 0.0 as dropout in LLMs is not recommended anymore\n",
    "            \"qkv_bias\": False        # Query-key-value bias\n",
    "        }\n",
    "\n",
    "    else:\n",
    "        GPT_CONFIG_124M = {\n",
    "            \"vocab_size\": 50257,     # Vocabulary size\n",
    "            \"context_length\": 1024,  # Context length\n",
    "            \"emb_dim\": 768,          # Embedding dimension\n",
    "            \"n_heads\": 12,           # Number of attention heads\n",
    "            \"n_layers\": 12,          # Number of layers\n",
    "            \"drop_rate\": 0.1,        # Dropout rate\n",
    "            \"qkv_bias\": False        # Query-key-value bias\n",
    "        }\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    torch.manual_seed(123)\n",
    "    model = GPTModel(GPT_CONFIG_124M)\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=0.1)\n",
    "    tokenizer = tiktoken.get_encoding(\"gpt2\")\n",
    "\n",
    "    data_dir = args.data_dir\n",
    "    all_files = [os.path.join(path, name) for path, subdirs, files\n",
    "                 in os.walk(data_dir) for name in files if name.endswith((\".txt\"))]\n",
    "    total_files = len(all_files)\n",
    "\n",
    "    if total_files == 0:\n",
    "        print(\"No training text files found. Make sure you \"\n",
    "              \"selected the correct input directory\")\n",
    "        quit()\n",
    "    print(\"Total files:\", total_files)\n",
    "\n",
    "    output_dir = Path(args.output_dir)\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    train_losses, val_losses, tokens_seen = train_model_simple(\n",
    "        model, optimizer, device,\n",
    "        batch_size=args.batch_size,\n",
    "        n_epochs=args.n_epochs,\n",
    "        eval_freq=args.eval_freq,\n",
    "        eval_iter=1,\n",
    "        print_sample_iter=args.print_sample_iter,\n",
    "        output_dir=output_dir,\n",
    "        save_ckpt_freq=args.save_ckpt_freq,\n",
    "        start_context=\"Every effort moves you\",\n",
    "        tokenizer=tokenizer\n",
    "    )\n",
    "\n",
    "    epochs_tensor = torch.linspace(0, args.n_epochs, len(train_losses))\n",
    "    plot_losses(epochs_tensor, tokens_seen, train_losses, val_losses, output_dir)\n",
    "\n",
    "    torch.save(model.state_dict(), output_dir / \"model_pg_final.pth\")\n",
    "    print(f\"Maximum GPU memory allocated: {torch.cuda.max_memory_allocated() / 1e9:.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
